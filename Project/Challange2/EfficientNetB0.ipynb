{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "EfficientNetB0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QosNj8a82A1x"
      },
      "source": [
        "# !git clone https://github.com/hussien/cifar10_pytorch.git"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqaglsA9ZYgf"
      },
      "source": [
        "# ! rm -r '/content/cifar10_pytorch'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viVdgJWU2guS"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqndcWxK2o_n"
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp54_tFD4g78",
        "outputId": "a4d83470-0e61-45dc-9d8c-ee487e68fa78"
      },
      "source": [
        "!pip install yacs"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (0.1.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from yacs) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgvlNlHj2yGn"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import os, sys\n",
        "from cifar10_pytorch.cifar10_pytorch.engine import train, test\n",
        "from cifar10_pytorch.cifar10_pytorch.config import cfg\n",
        "from cifar10_pytorch.cifar10_pytorch.data import dataset\n",
        "from cifar10_pytorch.cifar10_pytorch.modeling import build_model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvpWCHKM2183"
      },
      "source": [
        "# ! rm -r 'CINIC-10-Filtered_1K'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t5n4DSVEIai"
      },
      "source": [
        "# !unzip -q  '/content/cifar10_pytorch/CINIC-10-Filtered_1K.zip' -d '/content/'\n",
        "# !unzip -q  '/content/cifar10_pytorch/train.zip' -d '/content/'\n",
        "# !unzip -q  '/content/cifar10_pytorch/test.zip' -d '/content/'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owpdAAGR_Ako"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIbrxq3T8fSA"
      },
      "source": [
        "def arg_parser():\n",
        "    parser = argparse.ArgumentParser(description=\"CIFAR10 training\")\n",
        "    parser.add_argument(\n",
        "        \"--config\",\n",
        "        default='/content/cifar10_pytorch/configs/efficientnetB0.yaml',\n",
        "        # default='/content/cifar10_pytorch/configs/efficientnetB4.yaml',\n",
        "        help=\"path to config file\",\n",
        "        type=str\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--tfboard', help='tensorboard path for logging', type=str, default='out')\n",
        "    parser.add_argument('--checkpoint_dir', type=str,\n",
        "                        default='checkpoints',\n",
        "                        help='directory where checkpoint files are saved')\n",
        "    parser.add_argument('--resume', type=str,\n",
        "                        default=None,\n",
        "                        help='checkpoint file path')\n",
        "    return parser.parse_args(\"\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfJDsbgGDgxl",
        "outputId": "4e39cb4d-61a5-41b3-a907-97541d445f54"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (56.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOvWWEZc28q2",
        "outputId": "4ad3babd-0f38-4e14-f843-56319e8fd1a4"
      },
      "source": [
        "# args and configs\n",
        "args = arg_parser()\n",
        "if args.config:\n",
        "    cfg.merge_from_file(args.config)\n",
        "cfg.freeze()\n",
        "os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Model definition\n",
        "# cfg.SOLVER.BATCHSIZE=64\n",
        "model = build_model(cfg)\n",
        "# cfg.MODEL.DEVICE='cpu'\n",
        "print(cfg.MODEL.DEVICE)\n",
        "device = cfg.MODEL.DEVICE\n",
        "model.to(device)\n",
        "# Build dataloader\n",
        "dataloader_train, dataloader_test = dataset.prepare_cifar10_dataset(cfg)\n",
        "print(len(dataloader_train.dataset))\n",
        "\n",
        "# Optimizer settings\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),\n",
        "                            lr=cfg.SOLVER.BASE_LR,\n",
        "                            momentum=cfg.SOLVER.MOMENTUM,\n",
        "                            weight_decay=cfg.SOLVER.WEIGHT_DECAY,\n",
        "                            )\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    cfg.SOLVER.MILESTONES,\n",
        "    cfg.SOLVER.GAMMA,\n",
        ")\n",
        "if args.resume:\n",
        "    checkpoint = torch.load(args.resume)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    #scheduler.last_epoch = checkpoint['epoch']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "else:\n",
        "    start_epoch = 0\n",
        "\n",
        "# Tensorboard settings\n",
        "if args.tfboard:\n",
        "    print(\"using tfboard\")\n",
        "    from tensorboardX import SummaryWriter\n",
        "    tblogger = SummaryWriter(args.tfboard)\n",
        "else:\n",
        "    tblogger = None\n",
        "\n",
        "# Training Loop\n",
        "# end_epoch = cfg.SOLVER.END_EPOCH\n",
        "end_epoch = 30\n",
        "train_loss_epoch,test_loss_epoch=[],[]\n",
        "train_acc_epoch,test_acc_epoch=[],[]\n",
        "for epoch in range(start_epoch, end_epoch):\n",
        "    scheduler.step()\n",
        "    print(\"------ start epoch {} / {} with lr = {:.5f} -----\".format(\n",
        "        epoch+1, end_epoch, scheduler.get_lr()[0])\n",
        "    )\n",
        "    \n",
        "    train_loss, train_acc = train(model, device, dataloader_train, criterion, optimizer, epoch)\n",
        "    train_loss_epoch.append(train_loss)\n",
        "    train_acc_epoch.append(train_acc)\n",
        "    if epoch%5 == 0:\n",
        "      test_loss, test_acc = test(model, device, dataloader_test, criterion, epoch)\n",
        "    test_loss_epoch.append(test_loss)\n",
        "    test_acc_epoch.append(test_acc)\n",
        "    if tblogger:\n",
        "        tblogger.add_scalar('train/loss', train_loss, epoch+1)\n",
        "        tblogger.add_scalar('train/acc', train_acc, epoch+1)\n",
        "        tblogger.add_scalar('test/acc', test_acc, epoch+1)\n",
        "    torch.save({'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                },\n",
        "                os.path.join(args.checkpoint_dir, \"snapshot.ckpt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "10100\n",
            "using tfboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ start epoch 1 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 1, iter 100 / 158, Loss: 1.164 | Acc: 61.750\n",
            "\n",
            "test set size= 157  batch size= 64\n",
            "test-> epoch 1, iter 100 / 157, Loss: 0.843 | Acc: 70.938\n",
            "Accuracy: 70.5500\n",
            "------ start epoch 2 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 2, iter 100 / 158, Loss: 0.500 | Acc: 82.047\n",
            "------ start epoch 3 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 3, iter 100 / 158, Loss: 0.363 | Acc: 87.266\n",
            "------ start epoch 4 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 4, iter 100 / 158, Loss: 0.278 | Acc: 90.469\n",
            "------ start epoch 5 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 5, iter 100 / 158, Loss: 0.214 | Acc: 92.391\n",
            "------ start epoch 6 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 6, iter 100 / 158, Loss: 0.169 | Acc: 94.234\n",
            "\n",
            "test set size= 157  batch size= 64\n",
            "test-> epoch 6, iter 100 / 157, Loss: 0.681 | Acc: 80.250\n",
            "Accuracy: 80.5900\n",
            "------ start epoch 7 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 7, iter 100 / 158, Loss: 0.131 | Acc: 95.547\n",
            "------ start epoch 8 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 8, iter 100 / 158, Loss: 0.110 | Acc: 96.281\n",
            "------ start epoch 9 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 9, iter 100 / 158, Loss: 0.098 | Acc: 96.688\n",
            "------ start epoch 10 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 10, iter 100 / 158, Loss: 0.080 | Acc: 97.359\n",
            "------ start epoch 11 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 11, iter 100 / 158, Loss: 0.069 | Acc: 97.625\n",
            "\n",
            "test set size= 157  batch size= 64\n",
            "test-> epoch 11, iter 100 / 157, Loss: 0.650 | Acc: 82.312\n",
            "Accuracy: 82.3700\n",
            "------ start epoch 12 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 12, iter 100 / 158, Loss: 0.068 | Acc: 97.938\n",
            "------ start epoch 13 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 13, iter 100 / 158, Loss: 0.062 | Acc: 98.031\n",
            "------ start epoch 14 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 14, iter 100 / 158, Loss: 0.062 | Acc: 97.797\n",
            "------ start epoch 15 / 30 with lr = 0.01500 -----\n",
            "\n",
            "train set size= 158  batch size= 64\n",
            "train-> epoch 15, iter 100 / 158, Loss: 0.060 | Acc: 98.016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90pvnC5UCdx6"
      },
      "source": [
        "import datetime \n",
        "currentDT = datetime.datetime.now()\n",
        "torch.save({'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss_epoch': train_loss_epoch,\n",
        "            'test_loss_epoch': test_loss_epoch,\n",
        "            'train_acc_epoch': train_acc_epoch,\n",
        "            'test_acc_epoch': test_acc_epoch\n",
        "            },\n",
        "            os.path.join(\"/content/\", \"EfficientNetB0_CINIC10_30epoch_\"+str(currentDT)+\".ckpt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K2X8O_r8iut"
      },
      "source": [
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "        for batch_i, (inputs, targets) in enumerate(dataloader_test):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            if batch_i > 0 and (batch_i + 1) % 100 == 0:\n",
        "                print(\"test-> epoch {}, iter {} / {}, Loss: {:.3f} | Acc: {:.3f}\".format(\n",
        "                    epoch+1,\n",
        "                    batch_i+1,\n",
        "                    len(dataloader_test),\n",
        "                    test_loss/(batch_i+1),\n",
        "                    100.*correct/total,\n",
        "                ))\n",
        "acc = 100.*correct/total\n",
        "print(\"Accuracy: {:.4f}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pukpk1TFo5J"
      },
      "source": [
        "inputs.detach()\n",
        "targets.detach()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXdS_CDWGU94"
      },
      "source": [
        "# loss_train = history.history['train_loss']\n",
        "# loss_val = history.history['val_loss']\n",
        "loss_train=train_loss_epoch\n",
        "loss_test=test_loss_epoch\n",
        "epochs = range(1,len(train_loss_epoch)+1)\n",
        "plt.plot(epochs, loss_train, 'b', label='CINIC Train loss')\n",
        "plt.plot(epochs, loss_test, 'g', label='MNIST Test loss')\n",
        "plt.title('Training and Test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "epochs = range(1,len(train_acc_epoch)+1)\n",
        "plt.plot(epochs, train_acc_epoch, 'b', label='CINIC Train Acc')\n",
        "plt.plot(epochs, test_acc_epoch, 'g', label='MNIST Test Acc')\n",
        "plt.title('Training and test Acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw2HeCPzJLdP"
      },
      "source": [
        "# !unzip -q  '/content/gdrive/MyDrive/DLCourseProject/train.zip' -d '/content/train'\n",
        "!unzip -q  '/content/gdrive/MyDrive/DL_Course/test.zip' -d '/content/test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTZPf8vEEwDd"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgb5lsHuIIe2"
      },
      "source": [
        "from cifar10_pytorch.cifar10_pytorch.data import transform\n",
        "validation_img_paths = []\n",
        "transform_test = transform.build_transforms(cfg, is_train=False)\n",
        "for i in range(1,51):\n",
        "  validation_img_paths.append(\"/content/test/all/\"+str(i).zfill(4)+\".png\")\n",
        "img_list = [Image.open( img_path) for img_path in validation_img_paths]\n",
        "\n",
        "validation_batch = torch.stack([transform_test(img).to(device)\n",
        "                                for img in img_list])\n",
        "\n",
        "pred_logits_tensor = model(validation_batch)\n",
        "pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n",
        "classes=['airplane','automobile','bird', 'cat','deer', 'dog', 'frog','horse', 'ship',  'truck']\n",
        "fig, axs = plt.subplots(10, 5, figsize=(10, 20))\n",
        "for i, img in enumerate(img_list):\n",
        "    ax = axs[int(i/5),i%5]\n",
        "    ax.axis('off')\n",
        "    # print(pred_probs[i])\n",
        "    ax.set_title(str(np.argmax(pred_probs[i]))+\" \"+classes[np.argmax(pred_probs[i])])                                                        \n",
        "    ax.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui5PM5qTQbAM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}