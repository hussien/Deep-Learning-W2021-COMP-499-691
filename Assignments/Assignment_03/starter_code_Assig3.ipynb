{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "starter_code_Assig3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmxbplCvicKi"
      },
      "source": [
        "Use the below model for **1 (a) - (d)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejUEe-M5kWjh"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "feature_model = nn.Sequential(nn.Conv2d(1, 32, 5), nn.BatchNorm2d(32), nn.ReLU(), \n",
        "                      nn.MaxPool2d(2, stride=2),\n",
        "                      nn.Conv2d(32, 64, 5), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                      nn.Conv2d(64, 64, 3), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                      nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())\n",
        "\n",
        "# For (b)-(d) add the task heads on top of the feature_model\n",
        "# Note this model can adapt the averaging to the size so inputs of 32x32 and 28x28 both work\n",
        "# Grayscale conversion for SVHN, you may use transforms.Grayscale(num_output_channels=1) found in torchvision\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx1hM74BmJa6"
      },
      "source": [
        "***Question*** **1 (e/f)**\n",
        "\n",
        "In this question we will train a joint embedding between a model embedding from MNIST and a model embedding from SVHN dataset, both digit datasets. Your specific task to evaluate this will be to try to obtain $50\\%$ or higher accuracy on the MNIST classification by embedding MNIST test digits and then searching for the 1-nearest neighbor SVHN digit and using it's category to classify.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJFa2_xgig0D"
      },
      "source": [
        "First we will define the mnist and svhn models. For svhn we will use a pre-trained model that can already classify svhn digits. The models are defined below "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTGR9-8EnJrG"
      },
      "source": [
        "## MNIST model\n",
        "model_mnist = nn.Sequential(nn.Conv2d(1, 32, 5), nn.BatchNorm2d(32), nn.ReLU(), #For (e) use SVHN nn.Conv2d(3,32,5)\n",
        "                      nn.MaxPool2d(2, stride=2),\n",
        "                      nn.Conv2d(32, 64, 5), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                      nn.Conv2d(64, 64, 3), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                      nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())\n",
        "\n",
        "\n",
        "### SVHN model, we will download one that is already trained to clasify svhn digits\n",
        "model_urls = {\n",
        "    'svhn': 'http://ml.cs.tsinghua.edu.cn/~chenxi/pytorch-models/svhn-f564f3d8.pth',\n",
        "}\n",
        "\n",
        "class SVHN(nn.Module):\n",
        "    def __init__(self, features, n_channel, num_classes):\n",
        "        super(SVHN, self).__init__()\n",
        "        assert isinstance(features, nn.Sequential), type(features)\n",
        "        self.features = features\n",
        "\n",
        "        #We won't use this classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(n_channel, num_classes)\n",
        "        )\n",
        "        print(self.features)\n",
        "        print(self.classifier)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for i, v in enumerate(cfg):\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            padding = v[1] if isinstance(v, tuple) else 1\n",
        "            out_channels = v[0] if isinstance(v, tuple) else v\n",
        "            conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(out_channels, affine=False), nn.ReLU(), nn.Dropout(0.3)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(), nn.Dropout(0.3)]\n",
        "            in_channels = out_channels\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def svhn_model(n_channel, pretrained=None):\n",
        "    cfg = [n_channel, n_channel, 'M', 2*n_channel, 2*n_channel, 'M', 4*n_channel, 4*n_channel, 'M', (8*n_channel, 0), 'M']\n",
        "    layers = make_layers(cfg, batch_norm=True)\n",
        "    model = SVHN(layers, n_channel=8*n_channel, num_classes=10)\n",
        "    if pretrained is not None:\n",
        "        m = model_zoo.load_url(model_urls['svhn'])\n",
        "        state_dict = m.state_dict() if isinstance(m, nn.Module) else m\n",
        "        assert isinstance(state_dict, (dict, OrderedDict)), type(state_dict)\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "base_svhn = svhn_model(n_channel=32,pretrained=True).features\n",
        "svhn_to_joint = nn.Linear(256,64)\n",
        "\n",
        "model_svhn = nn.Sequential(base_svhn, nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), svhn_to_joint)\n",
        "\n",
        "\n",
        "#Transformation for SVHN data, you need to use this normalization for the pre-trained model to work properly \n",
        "transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw8ArWWDmTnc"
      },
      "source": [
        "Let's denote model_mnist above as $f_{\\theta}(x)$, the pre_trained model $g_{\\gamma}$ and svhn_to_joint as the matrix $W$. Finally model_svhn corresponds to $WAg_{\\gamma}(x)$. Here A (nn.AdaptiveAvgPool2d) is the averaging operator and has no parameters. Thus model_svhn will map svhn digits to a joint space and model_mnist will map MNIST digits to the joint space.  We will keep $g_{\n",
        "\\gamma}$ fixed and update $\\theta, W$.  You should optimize the following objective that is a sum of two loss functions over triplets\n",
        "\n",
        "\n",
        "$$\\min_{\\theta, W} \\sum_{x_a,x_p,x_n \\in \\textbf{M}} max(0, \\|f_{\\theta}(x_a) - WAg_{\\gamma}(x_p) \\|-\\|f_{\\theta}(x_a) - WAg_{\\gamma}(x_n) \\|+\\alpha) +\\sum_{x_a,x_p,x_n \\in \\textbf{S}} max(0, \\|f_{\\theta}(x_p) - WAg_{\\gamma}(x_a) \\|-\\|f_{\\theta}(x_n) - WAg_{\\gamma}(x_a) \\|+\\alpha)$$ \n",
        "\n",
        "\n",
        "Here $\\textbf{M}$ is the set of triplets with anchors from MNIST data, positives from SVHN (matching the anchor class), and negatives from SVHN (with different class from anchors). Similarly $\\textbf{S}$ is the set of triplets with anchors from SVHN data, positives from MNIST (matching anchor class), and negatives from MNIST not matching anchor class. You can use nn.TripletMarginLoss to implement this.  \n",
        "\n",
        "During training with a stochastic optimizer we will sample subsets of $M$ and $S$ for each gradient update, there are various valid ways to sample this as will be discussed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbJgYh-tIXPB"
      },
      "source": [
        "Note we only optimize W and $\\theta$, below see an example how to build the optimizer. Note we want to freeze the $g_{\\gamma}$ model so we will also need to disable the dropout and batchnorm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yXPy7ubjEs0"
      },
      "source": [
        "optimizer = optim.Adam(list(model_m.parameters()) + list(svhn_to_joint.parameters()), lr=1e-5) # you may experiment with different learning rates\n",
        "model_svhn.eval() #IMPORTANT: BEFORE running set to eval even for training to avoid dropout, we want to keep this fixed except the final layer, otherwise training will need to be much longer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unQC9RfEmBHY"
      },
      "source": [
        "**Suggested settings:** learning rate 1e-5 with Adam, margin ($\\alpha$) of 0.2, batch size: 256 triplets samples $M$ and 256 from $S$, 1000 training iterations (not epochs, but gradient updates/minibatch processed, aka it can be trained fast!). You may modify these as you see fit.\n",
        "\n",
        "Data augmentation is not required to make this work but you may use it if you like. For SVHN you must use the normalization above (transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))) so that the pre-trained SVHN model works. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7rzL6W7ocCF"
      },
      "source": [
        "**Sampling the triplets** There are various valid ways you could construct the triplet sets $\\textbf{M, S}$ and sample from them. For example you could enumerate all possible triplets over the dataset and select batches of these. A quick and dirty on the fly method that allows to use standard dataloaders is as follows: Sample a minibatch of size N (say 256) from both SVHN and MNIST using standard dataloaders from classification tasks. Treat all SVHN digits in this batch as  anchors, from the MNIST minibatch data find appropriate positives and negatives for each SVHN digit. For the second part of the loss treat the MNIST data as anchors and find negatives and postivies from the SVHN minibatch. Partial code snippets to construct this is shown below (note this code would give triplets for $\\textbf{M}$ part only). You may also use your own approach to sample the triplet sets.\n",
        "\n",
        "Note: if you would like to use hard negative mining (not required) a more sophisticated approach would be needed. Below is a code snippet example of how one could pick the positives using the labels for each minibatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlC0QvRfpnhj"
      },
      "source": [
        "#s_labels is a vector with batch_size labels (0-9) for a minibatch of SVHN digits\n",
        "#m_labels is a vector with batch_size labels (0-9) for a minibatch of MNIST digits \n",
        "\n",
        "label_set = range(0,9)\n",
        "label_to_indices = {label: np.where(s_labels.cpu().numpy() == label)[0]\n",
        "                            for label in label_set}\n",
        "\n",
        "                          \n",
        "idx_pos = []\n",
        "idx_neg = []\n",
        "for lab in m_labels:\n",
        "    positive_index = np.random.choice(label_to_indices[lab.item()])\n",
        "    negative_label = np.random.choice(list(set(label_set) - set([lab.item()])))\n",
        "    negative_index = np.random.choice(label_to_indices[negative_label])\n",
        "\n",
        "    idx_pos.append(positive_index)\n",
        "    idx_neg.append(negative_index)\n",
        "\n",
        "#idx_pos and idx_neg can now can now be used to index the MNIST data minibatch to give positives and negatives\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFIYMUcropPP"
      },
      "source": [
        "**Evaluation** For evaluating your embeddings use 2000 randomly selected SVHN digits from the SVHN training set embedding them with model_svhn. Use 100 randomly selected MNIST digits from the MNIST TEST set embedding them with model_mnist. The above numbers are chosen to avoid memory issues and reduce computation time, you may use larger amount of test inputs and embeddings if you wish. Assume the category data for the SVHN data is known and find for each MNIST digit the nearest SVHN digit. Report it's category as the prediction and compute the accuracy over all 100 MNIST digits. You should be able to obtain at least 50%+ although much higher accuracy is possible with a well tuned model. \n",
        "\n",
        "Finally for 3-5 MNIST digits show the top 5 SVHN sorted by lowest distance. (now extra credit but easy if the model works)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2LuRev-M_oE"
      },
      "source": [
        "If you run into memory issues you can move your model to CPU to process the SVHN encodings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ocFbnjojmD"
      },
      "source": [
        "**Question Grading** If you have trouble getting this to work you may still get partial credit for appropriate methodology. Grading for this question will be as follows:\n",
        "\n",
        "10 points - appropriate triplet construction and loss function construction\n",
        "\n",
        "10 points - appropriate nearest neighbor classification evaluation setup \n",
        "\n",
        "10 points - obtaining above 50% accuracy, 5 points for getting above 25%\n",
        "\n",
        "5 points (extra credit) - visualization of the retrieval\n",
        "\n",
        "5 points (extra credit) - hard negative mining\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWfYqk06SkJg"
      },
      "source": [
        "You can include your answer in a separate notebook or .py file"
      ]
    }
  ]
}