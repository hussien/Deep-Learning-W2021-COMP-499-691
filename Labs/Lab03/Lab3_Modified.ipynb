{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72veG8oHuZP1"
   },
   "source": [
    "# Lab 3 - Batch Normalization\n",
    "\n",
    "This lab has the following goals:\n",
    "a) Implement functional and module based batch normalization layer b) Understand the subtleties regarding batchnorm usage, particularly avoiding statistic computation in the test set c) Introduce the use of `register_buffer` in `torch.nn.Module` d) Understand the `.eval()` and `.train()` methods of `torch.nn.Module` and what these do. It is recommended to run the lab mini-experiments on GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4bggO7azKQo"
   },
   "source": [
    "## 0 Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIDPxv5l2qpI"
   },
   "source": [
    "Run the code cells below to initialize the train and test loaders of the MNIST dataset and visualize one of the MNIST samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4ZJYEyoxVPsX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9c1ac0c671494791796de5a85e5eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c56f6a16cc4d3ab52425677b102b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f2d431ddec49418345c00ef48d90e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077c7ab584484a309148a56d41fb7f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\torchvision\\datasets\\mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets,transforms\n",
    "\n",
    "\n",
    "# Initialize train and test datasets\n",
    "train_set = datasets.MNIST('../data',\n",
    "                           train=True,\n",
    "                           download=True,\n",
    "                           transform=transforms.ToTensor())\n",
    "test_set = datasets.MNIST('../data',\n",
    "                          train=False,\n",
    "                          download=True,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "# Initialize train and test data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size=256,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                          batch_size=256,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TPtGQ5qC1ihG"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize a sample from MNIST\n",
    "X_train_samples, y_train_samples = next(iter(train_loader))\n",
    "plt.title(f'Label: {y_train_samples[0]}')\n",
    "plt.imshow((X_train_samples[0].squeeze(0)).numpy(), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W37VSATWz5RW"
   },
   "source": [
    "## 1 Functional Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jCibZUmN7-q"
   },
   "source": [
    "### 1.1 Batch Normalization Function\n",
    "Implement a function that performs batch normalization on a given `inputs` tensor of shape `(N, F)`, where `N` is the minibatch size and `F` is the number of features. Note that batch normalization performs differently at train and inference time:\n",
    "* `train`: During training, batch normalization standardizes the given inputs along the minibatch dimension (mean and standard deviation would be of shape `(F,)`). The running average of the minibatch means and variances are updated during training. Learnable parameters $\\beta$ and $\\gamma$ shift and scale the distribution after standardization.\n",
    "* `eval`: During evaluation (inference), batch normalization uses the running average of the means and standard deviations which were computed during training for normalization.\n",
    "\n",
    "Implement a functional batch normalization layer with the differentiable affine parameters $\\gamma$ and $\\beta$. The batch normalization layer has the following formulation:\n",
    "\n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "\n",
    "You will need to create an additional set of variables to track and update the statistics (`toy_stats_dict`). Note that the statistics are updated outside of backpropagation. For the momentum rate of batchnorm statistics use `0.1`.\n",
    "\n",
    "Your function is then checked in train mode with 100 sample random values $\\sim \\mathcal{N}(50,10)$ (so shape would be `(100, 1)`. The correct printed output should be (very close to):\n",
    "\n",
    "    Training Samples\n",
    "    Before BN: mean tensor([54.8908]), var tensor([8.1866])\n",
    "    After BN: mean tensor([-1.3208e-06], grad_fn=<MeanBackward1>), var tensor([0.9999], grad_fn=<VarBackward1>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "K8wWItx6B_lY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Training Samples\n",
      "Before BN: mean tensor([54.8908]), var tensor([8.1866])\n",
      "After BN: mean tensor([-2.6489e-06], grad_fn=<MeanBackward1>), var tensor([1.0019], grad_fn=<VarBackward1>)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\ipykernel_launcher.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(691)\n",
    "\n",
    "# Number of features\n",
    "train_size = 500\n",
    "test_size = 1\n",
    "num_features = 1\n",
    "\n",
    "# Generates toy train features for evaluating your function down below\n",
    "toy_train_features = (torch.rand(train_size, num_features) * 10) + 50\n",
    "# print(toy_train_features)\n",
    "\n",
    "### TODO: Initialize the `running_mean` and `running_var` variables\n",
    "### with 0 and 1 values respectively.\n",
    "toy_stats_dict = {\n",
    "    \"running_mean\": 0,\n",
    "    \"running_var\": 1,\n",
    "}\n",
    "\n",
    "### TODO: Initialize the learnable parameters `beta` and `gamma` \n",
    "### with 0 and 1 values respectively.\n",
    "beta = 0\n",
    "gamma = 1\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def batchnorm(inputs,  beta,gamma, stats_dict,train=True, eps=0.001, momentum=0.1):\n",
    "    # Use `is_grad_enabled` to determine whether the current mode is training\n",
    "    # mode or prediction mode\n",
    "    moving_mean=stats_dict[\"running_mean\"]\n",
    "    moving_var=stats_dict[\"running_var\"]\n",
    "    if not train:\n",
    "        # If it is prediction mode, directly use the mean and variance\n",
    "        # obtained by moving average\n",
    "        inputs_hat = (inputs - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        print(len(inputs.shape))\n",
    "        assert len(inputs.shape) in (2, 4)\n",
    "        if len(inputs.shape) == 2:\n",
    "            # When using a fully-connected layer, calculate the mean and\n",
    "            # variance on the feature dimension\n",
    "            mean = inputs.mean(dim=0)\n",
    "            var = ((inputs - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # When using a two-dimensional convolutional layer, calculate the\n",
    "            # mean and variance on the channel dimension (axis=1). Here we\n",
    "            # need to maintain the shape of `X`, so that the broadcasting\n",
    "            # operation can be carried out later\n",
    "            mean = inputs.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((inputs - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # In training mode, the current mean and variance are used for the\n",
    "        # standardization\n",
    "        inputs_hat = (inputs - mean) / torch.sqrt(var + eps)\n",
    "        # Update the mean and variance using moving average\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = torch.tensor(gamma * inputs_hat + beta,requires_grad=True)  # Scale and shift\n",
    "   \n",
    "    return Y\n",
    "\n",
    "\n",
    "# run batchnorm on toy train features\n",
    "bn_out_train = batchnorm(toy_train_features, beta, gamma, toy_stats_dict)\n",
    "# print(bn_out_train)\n",
    "# print results\n",
    "print(\"Training Samples\")\n",
    "print(f\"Before BN: mean {toy_train_features.mean(0)}, var {toy_train_features.var(0)}\")\n",
    "print(f\"After BN: mean {bn_out_train.mean(0)}, var {bn_out_train.var(0)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14MBuiusdZmA"
   },
   "source": [
    "### 1.2 Setting up the Model Arhitecture\n",
    "For the model architecture, we use the 2 layer model from previous labs (the one that doesnt use `nn.Module`) and use the batchnorm function defined in part (1.1) (without the batchnorm parameters) at the 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "bhZ0I_q7dnb9"
   },
   "outputs": [],
   "source": [
    "# Initialize model hiden layer sizes\n",
    "h1_size = 50\n",
    "h2_size = 50\n",
    "\n",
    "### TODO: Initialize the beta and gamma parameters\n",
    "beta0 = 0\n",
    "gamma0 = 1\n",
    "beta1 = 0\n",
    "gamma1 = 1\n",
    "\n",
    "# Intentional naive initialization (do not modify)\n",
    "param_dict = {\n",
    "    \"W0\": torch.rand(784, h1_size)*2-1,\n",
    "    \"b0\": torch.rand(h1_size)*2-1,    \n",
    "    \"W1\": torch.rand(h1_size, h2_size)*2-1,\n",
    "    \"b1\": torch.rand(h2_size)*2-1,    \n",
    "    \"W2\": torch.rand(h2_size,10)*2-1,\n",
    "    \"b2\": torch.rand(10)*2-1,\n",
    "}\n",
    "for name, param in param_dict.items():\n",
    "#     param_dict[name] = param.to(device)\n",
    "    param_dict[name] = param\n",
    "    param.requires_grad=True\n",
    "param_dict[\"beta0\"]= beta0\n",
    "param_dict[\"beta1\"]= beta1\n",
    "param_dict[\"gamma0\"]= gamma0\n",
    "param_dict[\"gamma1\"]= gamma1\n",
    "### TODO: Initialize the `running_mean` and `running_var` variables\n",
    "### with 0s and 1s respectively.\n",
    "l1_stats_dict = {\n",
    "    \"running_mean\": 0,\n",
    "    \"running_var\": 1,\n",
    "}\n",
    "l2_stats_dict = {\n",
    "    \"running_mean\": 0,\n",
    "    \"running_var\": 1,\n",
    "}\n",
    "layers_stats_list = [l1_stats_dict, l2_stats_dict]\n",
    "\n",
    "def my_nn(input, param_dict, layers_stats_list, train=True):\n",
    "    r\"\"\"Performs a single forward pass of a 2 layer MLP with batch \n",
    "    normalization using the given parameters in param_dict and the\n",
    "    batch norm statistics in layers_stats_list.\n",
    "\n",
    "    Args:\n",
    "        input (torch.tensor): Batch of images of shape (N, H, W), where N is \n",
    "            the number of input samples, and H and W are the image height and \n",
    "            width respectively.\n",
    "        param_dict (dict of torch.tensor): Dictionary containing the parameters\n",
    "            of the neural network. Expects dictionary keys to be of format \n",
    "            \"W#\", \"b#\", \"beta#\" and \"gamma#\" where # is the layer number.\n",
    "        layers_stats_list (list of dict of torch.tensor): List of dictionaries\n",
    "            containing running means and variances for each layer. List size \n",
    "            is equal to the number of hidden layers.\n",
    "        train (bool): Determines whether batch norm is in train mode or not.\n",
    "            Default: True\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: Neural network output of shape (N, 10)\n",
    "    \"\"\"\n",
    "    x = input.view(-1, 28*28) \n",
    "\n",
    "    # layer 1\n",
    "    x = torch.relu_(x @ param_dict['W0'] + param_dict['b0'])\n",
    "\n",
    "    ### TODO: use your complete batchnorm function\n",
    "    x = batchnorm(x, beta0, gamma0, l1_stats_dict)\n",
    "\n",
    "    # layer 2\n",
    "    x = torch.relu_(x @ param_dict['W1'] + param_dict['b1'])\n",
    "\n",
    "    ### TODO: use your complete batchnorm function\n",
    "    x = batchnorm(x, beta1, gamma1, l2_stats_dict)\n",
    "    # output \n",
    "    x = x @ param_dict['W2'] + param_dict['b2']\n",
    "    return x\n",
    "\n",
    "def my_zero_grad(param_dict):\n",
    "    r\"\"\"Zeros the gradients of the parameters in `param_dict`.\n",
    "\n",
    "    Args:\n",
    "        param_dict (dict of torch.tensor): Dictionary containing the parameters\n",
    "            of the neural network. Expects dictionary keys to be of format \n",
    "            \"W#\", \"b#\", \"beta#\" and \"gamma#\" where # is the layer number.\n",
    "        layers_stats_list (list of dict of torch.tensor): List of dictionaries\n",
    "            containing running means and variances for each layer. List size \n",
    "            is equal to the number of hidden layers.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for _, param in param_dict.items():\n",
    "        if param.grad is not None: \n",
    "            param.grad.detach_()\n",
    "            param.grad.zero_()\n",
    "\n",
    "            \n",
    "def initialize_nn(param_dict, layers_stats_list):\n",
    "    r\"\"\"Initializes the parameters in `param_dict` and resets the statistics\n",
    "    in `layers_stats_list`.\n",
    "\n",
    "    Args:\n",
    "        param_dict (dict of torch.tensor): Dictionary containing the parameters\n",
    "            of the neural network. Expects dictionary keys to be of format \n",
    "            \"W#\", \"b#\", \"beta#\" and \"gamma#\" where # is the layer number.\n",
    "        layers_stats_list (list of dict of torch.tensor): List of dictionaries\n",
    "            containing running means and variances for each layer. List size \n",
    "            is equal to the number of hidden layers.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSb7w9p-vGlQ"
   },
   "source": [
    "### 1.3 Training the Model\n",
    "\n",
    "Train the model on the MNIST dataset with 20 epochs and `lr=0.01` with SGD and without momentum (as per lab 2). Plot the learning curves for training accuracy recorded every 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "mDmMiqCAv36z"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-fa3860cb3d90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'params'"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# training hyper_parameters\n",
    "lr = 0.01\n",
    "num_epochs = 20\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data,label) in enumerate(train_loader):\n",
    "        ### TODO: Train the network\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38H0o2CzjBNa"
   },
   "source": [
    "### 1.4 Evaluating the Model\n",
    "\n",
    "Evaluate the model taking care that the statistics should not be used from the test set. Explain why the evaluation needs to be treated differently. Print the accuracy of both the train and test set in evaluation mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICebAfnmqyGS"
   },
   "outputs": [],
   "source": [
    "### TODO: Evaluate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndXpk3C5bwQ8"
   },
   "source": [
    "## 2 Modular Batch Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OSCgA_l0kMf"
   },
   "source": [
    "### 2.1 Batch Normalization Module\n",
    "\n",
    "Implement a `torch.nn.Module` that performs the batch normalization operation. You will need to use the `register_buffer` in the `__init__` call of your custom `nn.Module` class to create variables that are not in the computation graph but tracked by `nn.Module`. Registering the buffer statistics for example allows the tensor to be moved onto the gpu when `model.cuda()` is called.\n",
    "\n",
    "**Hint:** You can use the `.training` attribute of `torch.nn.Module` to detect if the model is in `.train()` mode or `.eval()` mode ([example](https://github.com/pytorch/pytorch/blob/fcf8b712348f21634044a5d76a69a59727756357/torch/nn/modules/batchnorm.py#L123))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nbgzjuqtlhvx"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class myBatchnorm(nn.Module):\n",
    "    def __init__(self, num_features, epsilon=1e-3, momentum=.1):\n",
    "        super(myBatchnorm,self).__init__()\n",
    "        self.epsilon = None\n",
    "        self.m = None\n",
    "\n",
    "        ### TODO: Initialize the `running_mean` and `running_var`\n",
    "        ### register buffers with 0s and 1s respectively.\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### TODO: perform batch normalization\n",
    "        ### HINT: use nn.Module's .training attribute\n",
    "        pass\n",
    "\n",
    "# Modify this class with your custom batchnorm\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, h1_siz, h2_siz):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, h1_siz)\n",
    "        self.linear2 = nn.Linear(h1_siz, h2_siz)\n",
    "        self.linear3 = nn.Linear(h2_siz, 10)\n",
    "\n",
    "        ### TODO: initialize batch normalization layers\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear1.weight.data.uniform_(-1, 1)\n",
    "        self.linear1.bias.data.uniform_(-1, 1)\n",
    "        self.linear2.weight.data.uniform_(-1, 1)\n",
    "        self.linear2.bias.data.uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        ### TODO: add batch normalization layer\n",
    "        \n",
    "        ###\n",
    "        x = self.linear2(F.relu(x))\n",
    "        ### TODO: add batch normalization layer\n",
    "\n",
    "        ###\n",
    "        x = F.relu(x)\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGuxRfgl1Le5"
   },
   "source": [
    "### 2.2 Training the Model\n",
    "\n",
    "Repeat training and overlay the training curves to those from (1.5) and validate it achieves similar test acc. In order to achieve the same behavior as your `train=False`/`train=True`,  you will need to use `.eval()` and `.train()` methods on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZ2TjMHB67n6"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, history_frequency=50):\n",
    "    r\"\"\"Iterates over train_loader and optimizes model using pre-initialized\n",
    "    optimizer.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to be trained\n",
    "        optimizer (torch.optim.Optimizer): initialized optimizer with lr and\n",
    "            model parameters\n",
    "        train_loader (torch.utils.data.DataLoader): Training set data loader\n",
    "        history_frequency (int): Frequency for the minibatch metrics to be \n",
    "            stored in minibatch_losses and minibatch_accuracies\n",
    "\n",
    "    Returns:\n",
    "        minibatch_losses (list of float): Minibatch loss every over the \n",
    "            training progress\n",
    "        minibatch_accuracies (list of float): Minibatch accuracy over the \n",
    "            training progress\n",
    "    \"\"\"\n",
    "    minibatch_losses = []\n",
    "    minibatch_accuracies = []\n",
    "\n",
    "    ### TODO: Use `.train()` to put model in training state\n",
    "\n",
    "    for i,(data,label) in enumerate(train_loader):\n",
    "        ### TODO: perform forward pass and backpropagation\n",
    "        ### TODO: store the loss and accuracy in `minibatch_losses` and \n",
    "        ### `minibatch_accuracies` every `history_frequency`th iteration\n",
    "        pass\n",
    "\n",
    "    return minibatch_losses, minibatch_accuracies\n",
    "\n",
    "def test(model, test_loader):\n",
    "    r\"\"\"Iterate over test_loader to compute the accuracy of the trained model\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to be evaluated\n",
    "        test_loader (torch.utils.data.DataLoader): Testing set data loader\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): Model accuracy on test set\n",
    "        loss (float): Model loss on test set\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "\n",
    "    ### TODO: Use `.eval()` to put model in evaluation state\n",
    "\n",
    "    for i,(data,label) in enumerate(test_loader):\n",
    "        ### TODO: perform forward pass and compute the loss and accuracy\n",
    "        pass\n",
    "\n",
    "    return (loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a07pnxWgyJyQ"
   },
   "outputs": [],
   "source": [
    "# training hyper_parameters\n",
    "lr = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "### TODO: initialize the model and the optimizer\n",
    "### Reminder: The running_mean and running_variance are not updated by gradient descent!\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, optimizer, train_loader)\n",
    "    train_losses.extend(train_loss)\n",
    "    train_accuracies.extend(train_accuracy)\n",
    "    test_loss, test_accuracy = test(model, test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVrfTlXb5Ezy"
   },
   "outputs": [],
   "source": [
    "### TODO: Visualize training curves "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkM2nBhBz3mm"
   },
   "source": [
    "### 2.3 PyTorch's nn.BatchNorm1d\n",
    "Finally repeat all these steps using PyTorch's [`nn.BatchNorm1d`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) module and validate that the training curves match those from (1.5) and (2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5otQmi3wzx4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Modify this class with your custom batchnorm\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, h1_siz, h2_siz):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, h1_siz)\n",
    "        self.linear2 = nn.Linear(h1_siz, h2_siz)\n",
    "        self.linear3 = nn.Linear(h2_siz, 1)\n",
    "        ### TODO: add batch normalization module\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear1.weight.data.uniform_(-1,1)\n",
    "        self.linear1.bias.data.uniform_(-1,1)\n",
    "        self.linear2.weight.data.uniform_(-1,1)\n",
    "        self.linear2.bias.data.uniform_(-1,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        ### TODO: add batch normalization layer\n",
    "        \n",
    "        ###\n",
    "        x = self.linear2(F.relu(x))\n",
    "        ### TODO: add batch normalization layer\n",
    "\n",
    "        ###\n",
    "        x = F.relu(x)\n",
    "        return self.linear3(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1NydTaw4-7N"
   },
   "outputs": [],
   "source": [
    "# training hyper_parameters\n",
    "lr = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "### TODO: initialize the model and the optimizer\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, optimizer, train_loader)\n",
    "    train_losses.extend(train_loss)\n",
    "    train_accuracies.extend(train_accuracy)\n",
    "    test_loss, test_accuracy = test(model, test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4CQLCYm5g6_"
   },
   "outputs": [],
   "source": [
    "### TODO: Visualize training curves "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3_Modified.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
