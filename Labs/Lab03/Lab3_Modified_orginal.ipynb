{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72veG8oHuZP1"
   },
   "source": [
    "# Lab 3 - Batch Normalization\n",
    "\n",
    "This lab has the following goals:\n",
    "a) Implement functional and module based batch normalization layer b) Understand the subtleties regarding batchnorm usage, particularly avoiding statistic computation in the test set c) Introduce the use of `register_buffer` in `torch.nn.Module` d) Understand the `.eval()` and `.train()` methods of `torch.nn.Module` and what these do. It is recommended to run the lab mini-experiments on GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4bggO7azKQo"
   },
   "source": [
    "## 0 Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIDPxv5l2qpI"
   },
   "source": [
    "Run the code cells below to initialize the train and test loaders of the MNIST dataset and visualize one of the MNIST samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZJYEyoxVPsX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets,transforms\n",
    "\n",
    "\n",
    "# Initialize train and test datasets\n",
    "train_set = datasets.MNIST('../data',\n",
    "                           train=True,\n",
    "                           download=True,\n",
    "                           transform=transforms.ToTensor())\n",
    "test_set = datasets.MNIST('../data',\n",
    "                          train=False,\n",
    "                          download=True,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "# Initialize train and test data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size=256,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                          batch_size=256,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPtGQ5qC1ihG"
   },
   "outputs": [],
   "source": [
    "# Visualize a sample from MNIST\n",
    "X_train_samples, y_train_samples = next(iter(train_loader))\n",
    "plt.title(f'Label: {y_train_samples[0]}')\n",
    "plt.imshow((X_train_samples[0].squeeze(0)).numpy(), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W37VSATWz5RW"
   },
   "source": [
    "## 1 Functional Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jCibZUmN7-q"
   },
   "source": [
    "### 1.1 Batch Normalization Function\n",
    "Implement a function that performs batch normalization on a given `inputs` tensor of shape `(N, F)`, where `N` is the minibatch size and `F` is the number of features. Note that batch normalization performs differently at train and inference time:\n",
    "* `train`: During training, batch normalization standardizes the given inputs along the minibatch dimension (mean and standard deviation would be of shape `(F,)`). The running average of the minibatch means and variances are updated during training. Learnable parameters $\\beta$ and $\\gamma$ shift and scale the distribution after standardization.\n",
    "* `eval`: During evaluation (inference), batch normalization uses the running average of the means and standard deviations which were computed during training for normalization.\n",
    "\n",
    "Implement a functional batch normalization layer with the differentiable affine parameters $\\gamma$ and $\\beta$. The batch normalization layer has the following formulation:\n",
    "\n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "\n",
    "You will need to create an additional set of variables to track and update the statistics (`toy_stats_dict`). Note that the statistics are updated outside of backpropagation. For the momentum rate of batchnorm statistics use `0.1`.\n",
    "\n",
    "Your function is then checked in train mode with 100 sample random values $\\sim \\mathcal{N}(50,10)$ (so shape would be `(100, 1)`. The correct printed output should be (very close to):\n",
    "\n",
    "    Training Samples\n",
    "    Before BN: mean tensor([54.8908]), var tensor([8.1866])\n",
    "    After BN: mean tensor([-1.3208e-06], grad_fn=<MeanBackward1>), var tensor([0.9999], grad_fn=<VarBackward1>)\n",
    "\n",
    "**Note:** To get these exact same values, your device would need to be set to cpu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8wWItx6B_lY"
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "torch.manual_seed(691)\n",
    "\n",
    "# Number of features\n",
    "train_size = 500\n",
    "test_size = 1\n",
    "num_features = 1\n",
    "\n",
    "# Generates toy train features for evaluating your function down below\n",
    "toy_train_features = (torch.rand(train_size, num_features) * 10) + 50\n",
    "\n",
    "### TODO: Initialize the `running_mean` and `running_var` variables\n",
    "### with 0 and 1 values respectively.\n",
    "toy_stats_dict = {\n",
    "    \"running_mean\": None,\n",
    "    \"running_var\": None,\n",
    "}\n",
    "\n",
    "### TODO: Initialize the learnable parameters `beta` and `gamma` \n",
    "### with 0 and 1 values respectively.\n",
    "beta = None\n",
    "gamma = None\n",
    "\n",
    "def batchnorm(inputs, beta, gamma, stats_dict, train=True, eps=0.001, momentum=0.1):\n",
    "    r\"\"\"Performs batch normalization for a single layer of inputs. If in train\n",
    "    mode, will update the stats_dict dictionary with running mean and variance\n",
    "    values.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.tensor): Batch of inputs of shape (N, F), where N is \n",
    "            the minibatch size, and F is the number of features.\n",
    "        beta (torch.tensor): Batch normalization beta variable of shape (F,).\n",
    "        gamma (torch.tensor): Batch normalization gamma variable of shape (F,).\n",
    "        stats_dict (dict of torch.tensor): Dictionary containing the running\n",
    "            mean and variance. Expects dictionary to contain keys 'running_mean'\n",
    "            and 'running_var', with values being `torch.tensor`s of shape (F,).\n",
    "        train (bool): Determines whether batch norm is in train mode or not.\n",
    "            Default: True\n",
    "        eps (float): Constant for numeric stability.\n",
    "        momentum (float): The momentum value for updating the running mean and\n",
    "            variance during training.\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: Batch normalized inputs, of shape (N, F)\n",
    "    \"\"\"\n",
    "    ### TODO: Fill out this function\n",
    "    pass\n",
    "\n",
    "# run batchnorm on toy train features\n",
    "bn_out_train = batchnorm(toy_train_features, beta, gamma, toy_stats_dict)\n",
    "\n",
    "# print results\n",
    "print(\"Training Samples\")\n",
    "print(f\"Before BN: mean {toy_train_features.mean(0)}, var {toy_train_features.var(0)}\")\n",
    "print(f\"After BN: mean {bn_out_train.mean(0)}, var {bn_out_train.var(0)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14MBuiusdZmA"
   },
   "source": [
    "### 1.2 Setting up the Model Arhitecture\n",
    "For the model architecture, we use the 2 layer model from previous labs (the one that doesnt use `nn.Module`) and use the batchnorm function defined in part (1.1) (without the batchnorm parameters) at the 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhZ0I_q7dnb9"
   },
   "outputs": [],
   "source": [
    "# Initialize model hiden layer sizes\n",
    "h1_size = 50\n",
    "h2_size = 50\n",
    "\n",
    "### TODO: Initialize the beta and gamma parameters\n",
    "beta0 = None\n",
    "gamma0 = None\n",
    "beta1 = None\n",
    "gamma1 = None\n",
    "\n",
    "# Intentional naive initialization (do not modify)\n",
    "param_dict = {\n",
    "    \"W0\": torch.rand(784, h1_size)*2-1,\n",
    "    \"b0\": torch.rand(h1_size)*2-1,\n",
    "    \"beta0\": beta0,\n",
    "    \"gamma0\": gamma0,\n",
    "    \"W1\": torch.rand(h1_size, h2_size)*2-1,\n",
    "    \"b1\": torch.rand(h2_size)*2-1,\n",
    "    \"beta1\": beta1,\n",
    "    \"gamma1\": gamma1,\n",
    "    \"W2\": torch.rand(h2_size,10)*2-1,\n",
    "    \"b2\": torch.rand(10)*2-1,\n",
    "}\n",
    "\n",
    "for name, param in param_dict.items():\n",
    "    param_dict[name] = param.to(device)\n",
    "    param_dict[name].requires_grad = True\n",
    "\n",
    "### TODO: Initialize the `running_mean` and `running_var` variables\n",
    "### with 0s and 1s respectively.\n",
    "l1_stats_dict = {\n",
    "    \"running_mean\": None,\n",
    "    \"running_var\": None,\n",
    "}\n",
    "l2_stats_dict = {\n",
    "    \"running_mean\": None,\n",
    "    \"running_var\": None,\n",
    "}\n",
    "layers_stats_list = [l1_stats_dict, l2_stats_dict]\n",
    "\n",
    "def my_nn(input, param_dict, layers_stats_list, train=True):\n",
    "    r\"\"\"Performs a single forward pass of a 2 layer MLP with batch \n",
    "    normalization using the given parameters in param_dict and the\n",
    "    batch norm statistics in layers_stats_list.\n",
    "\n",
    "    Args:\n",
    "        input (torch.tensor): Batch of images of shape (N, H, W), where N is \n",
    "            the number of input samples, and H and W are the image height and \n",
    "            width respectively.\n",
    "        param_dict (dict of torch.tensor): Dictionary containing the parameters\n",
    "            of the neural network. Expects dictionary keys to be of format \n",
    "            \"W#\", \"b#\", \"beta#\" and \"gamma#\" where # is the layer number.\n",
    "        layers_stats_list (list of dict of torch.tensor): List of dictionaries\n",
    "            containing running means and variances for each layer. List size \n",
    "            is equal to the number of hidden layers.\n",
    "        train (bool): Determines whether batch norm is in train mode or not.\n",
    "            Default: True\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: Neural network output of shape (N, 10)\n",
    "    \"\"\"\n",
    "    x = input.view(-1, 28*28) \n",
    "\n",
    "    # layer 1\n",
    "    x = torch.relu_(x @ param_dict['W0'] + param_dict['b0'])\n",
    "\n",
    "    ### TODO: use your complete batchnorm function\n",
    "\n",
    "    # layer 2\n",
    "    x = torch.relu_(x @ param_dict['W1'] + param_dict['b1'])\n",
    "\n",
    "    ### TODO: use your complete batchnorm function\n",
    "\n",
    "    # output \n",
    "    x = x @ param_dict['W2'] + param_dict['b2']\n",
    "    return x\n",
    "\n",
    "def my_zero_grad(param_dict):\n",
    "    r\"\"\"Zeros the gradients of the parameters in `param_dict`.\n",
    "\n",
    "    Args:\n",
    "        param_dict (dict of torch.tensor): Dictionary containing the parameters\n",
    "            of the neural network. Expects dictionary keys to be of format \n",
    "            \"W#\", \"b#\", \"beta#\" and \"gamma#\" where # is the layer number.\n",
    "        layers_stats_list (list of dict of torch.tensor): List of dictionaries\n",
    "            containing running means and variances for each layer. List size \n",
    "            is equal to the number of hidden layers.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for _, param in param_dict.items():\n",
    "        if param.grad is not None: \n",
    "            param.grad.detach_()\n",
    "            param.grad.zero_()\n",
    "\n",
    "def initialize_nn(param_dict, layers_stats_list):\n",
    "    r\"\"\"Initializes the parameters in `param_dict` and resets the statistics\n",
    "    in `layers_stats_list`.\n",
    "\n",
    "    Args:\n",
    "        param_dict (dict of torch.tensor): Dictionary containing the parameters\n",
    "            of the neural network. Expects dictionary keys to be of format \n",
    "            \"W#\", \"b#\", \"beta#\" and \"gamma#\" where # is the layer number.\n",
    "        layers_stats_list (list of dict of torch.tensor): List of dictionaries\n",
    "            containing running means and variances for each layer. List size \n",
    "            is equal to the number of hidden layers.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ### TODO: Fill out this function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSb7w9p-vGlQ"
   },
   "source": [
    "### 1.3 Training the Model\n",
    "\n",
    "Train the model on the MNIST dataset with 20 epochs and `lr=0.01` with SGD and without momentum (as per lab 2). Plot the learning curves for training accuracy recorded every 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDmMiqCAv36z"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# training hyper_parameters\n",
    "lr = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "### TODO: Initialize optimizer. You can use the SGD class from pytorch.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data,label) in enumerate(train_loader):\n",
    "        ### TODO: Train the network\n",
    "        pass\n",
    "    \n",
    "from itertools import repeat \n",
    "cuda = torch.cuda.is_available()\n",
    "acc=[]\n",
    "loss=[]\n",
    "batch_idx=0\n",
    "for data, targets in test_loader:\n",
    "  #move to GPU if available\n",
    "#   if cuda:\n",
    "#     data, target = data.to('cuda'), target.to('cuda')\n",
    "  #compute model output\n",
    "#     print(len(data))\n",
    "#     for i in range(len(data)):\n",
    "    Q=torch.zeros(len(data), 10)\n",
    "    for i in range(len(data)):\n",
    "        out=my_nn(data[i],param_dict)\n",
    "#         print(out.numpy())\n",
    "        Q[i,:]=out\n",
    "#         print(len(targets))\n",
    "  #comptue accuracy for minibatch\n",
    "    pred = torch.nn.functional.softmax(Q, dim=1)\n",
    "    total=0\n",
    "    for i, p in enumerate(pred):\n",
    "        if targets[i] == torch.max(p.data, 0)[1]:\n",
    "            total = total + 1\n",
    "    acc.append(total/len(data))\n",
    "  #compute loss for minibatch\n",
    "    loss.append(torch.nn.functional.cross_entropy(Q,targets))\n",
    "    print('batch: ',batch_idx,' loss=',loss[batch_idx],' acc=',acc[batch_idx])\n",
    "    batch_idx=batch_idx+1\n",
    "#aggregate loss and accuracy for all test data\n",
    "print('avg acc =',sum(acc)/len(acc))\n",
    "print('avg loss =',sum(loss)/len(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38H0o2CzjBNa"
   },
   "source": [
    "### 1.4 Evaluating the Model\n",
    "\n",
    "Evaluate the model taking care that the statistics should not be used from the test set. Explain why the evaluation needs to be treated differently. Print the accuracy of both the train and test set in evaluation mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICebAfnmqyGS"
   },
   "outputs": [],
   "source": [
    "### TODO: Evaluate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndXpk3C5bwQ8"
   },
   "source": [
    "## 2 Modular Batch Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OSCgA_l0kMf"
   },
   "source": [
    "### 2.1 Batch Normalization Module\n",
    "\n",
    "Implement a `torch.nn.Module` that performs the batch normalization operation. You will need to use the `register_buffer` in the `__init__` call of your custom `nn.Module` class to create variables that are not in the computation graph but tracked by `nn.Module`. Registering the buffer statistics for example allows the tensor to be moved onto the gpu when `model.cuda()` is called.\n",
    "\n",
    "**Hint:** You can use the `.training` attribute of `torch.nn.Module` to detect if the model is in `.train()` mode or `.eval()` mode ([example](https://github.com/pytorch/pytorch/blob/fcf8b712348f21634044a5d76a69a59727756357/torch/nn/modules/batchnorm.py#L123))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nbgzjuqtlhvx"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class myBatchnorm(nn.Module):\n",
    "    def __init__(self, num_features, epsilon=1e-3, momentum=.1):\n",
    "        super(myBatchnorm,self).__init__()\n",
    "        self.epsilon = None\n",
    "        self.m = None\n",
    "\n",
    "        ### TODO: Initialize the `running_mean` and `running_var`\n",
    "        ### register buffers with 0s and 1s respectively.\n",
    "\n",
    "        ### TODO: Initialize the gamma and beta parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### TODO: perform batch normalization\n",
    "        ### HINT: use nn.Module's .training attribute\n",
    "        pass\n",
    "\n",
    "# Modify this class with your custom batchnorm\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, h1_siz, h2_siz):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, h1_siz)\n",
    "        self.linear2 = nn.Linear(h1_siz, h2_siz)\n",
    "        self.linear3 = nn.Linear(h2_siz, 10)\n",
    "\n",
    "        ### TODO: initialize batch normalization layers\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear1.weight.data.uniform_(-1, 1)\n",
    "        self.linear1.bias.data.uniform_(-1, 1)\n",
    "        self.linear2.weight.data.uniform_(-1, 1)\n",
    "        self.linear2.bias.data.uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        ### TODO: add batch normalization layer\n",
    "        \n",
    "        ###\n",
    "        x = self.linear2(F.relu(x))\n",
    "        ### TODO: add batch normalization layer\n",
    "\n",
    "        ###\n",
    "        x = F.relu(x)\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGuxRfgl1Le5"
   },
   "source": [
    "### 2.2 Training the Model\n",
    "\n",
    "Repeat training and overlay the training curves to those from (1.5) and validate it achieves similar test acc. In order to achieve the same behavior as your `train=False`/`train=True`,  you will need to use `.eval()` and `.train()` methods on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZ2TjMHB67n6"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, history_frequency=50):\n",
    "    r\"\"\"Iterates over train_loader and optimizes model using pre-initialized\n",
    "    optimizer.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to be trained\n",
    "        optimizer (torch.optim.Optimizer): initialized optimizer with lr and\n",
    "            model parameters\n",
    "        train_loader (torch.utils.data.DataLoader): Training set data loader\n",
    "        history_frequency (int): Frequency for the minibatch metrics to be \n",
    "            stored in minibatch_losses and minibatch_accuracies\n",
    "\n",
    "    Returns:\n",
    "        minibatch_losses (list of float): Minibatch loss every over the \n",
    "            training progress\n",
    "        minibatch_accuracies (list of float): Minibatch accuracy over the \n",
    "            training progress\n",
    "    \"\"\"\n",
    "    minibatch_losses = []\n",
    "    minibatch_accuracies = []\n",
    "\n",
    "    ### TODO: Use `.train()` to put model in training state\n",
    "\n",
    "    for i,(data,label) in enumerate(train_loader):\n",
    "        ### TODO: perform forward pass and backpropagation\n",
    "        ### TODO: store the loss and accuracy in `minibatch_losses` and \n",
    "        ### `minibatch_accuracies` every `history_frequency`th iteration\n",
    "        pass\n",
    "\n",
    "    return minibatch_losses, minibatch_accuracies\n",
    "\n",
    "def test(model, test_loader):\n",
    "    r\"\"\"Iterate over test_loader to compute the accuracy of the trained model\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model to be evaluated\n",
    "        test_loader (torch.utils.data.DataLoader): Testing set data loader\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): Model accuracy on test set\n",
    "        loss (float): Model loss on test set\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "\n",
    "    ### TODO: Use `.eval()` to put model in evaluation state\n",
    "\n",
    "    for i,(data,label) in enumerate(test_loader):\n",
    "        ### TODO: perform forward pass and compute the loss and accuracy\n",
    "        pass\n",
    "\n",
    "    return (loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a07pnxWgyJyQ"
   },
   "outputs": [],
   "source": [
    "# training hyper_parameters\n",
    "lr = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "### TODO: initialize the model and the optimizer\n",
    "### Reminder: The running_mean and running_variance are not updated by gradient descent!\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, optimizer, train_loader)\n",
    "    train_losses.extend(train_loss)\n",
    "    train_accuracies.extend(train_accuracy)\n",
    "    test_loss, test_accuracy = test(model, test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVrfTlXb5Ezy"
   },
   "outputs": [],
   "source": [
    "### TODO: Visualize training curves "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkM2nBhBz3mm"
   },
   "source": [
    "### 2.3 PyTorch's nn.BatchNorm1d\n",
    "Finally repeat all these steps using PyTorch's [`nn.BatchNorm1d`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) module and validate that the training curves match those from (1.5) and (2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5otQmi3wzx4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Modify this class with your custom batchnorm\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, h1_siz, h2_siz):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, h1_siz)\n",
    "        self.linear2 = nn.Linear(h1_siz, h2_siz)\n",
    "        self.linear3 = nn.Linear(h2_siz, 10)\n",
    "        ### TODO: add batch normalization module\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear1.weight.data.uniform_(-1,1)\n",
    "        self.linear1.bias.data.uniform_(-1,1)\n",
    "        self.linear2.weight.data.uniform_(-1,1)\n",
    "        self.linear2.bias.data.uniform_(-1,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        ### TODO: add batch normalization layer\n",
    "        \n",
    "        ###\n",
    "        x = self.linear2(x)\n",
    "        ### TODO: add batch normalization layer\n",
    "\n",
    "        ###\n",
    "        x = F.relu(x)\n",
    "        return self.linear3(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1NydTaw4-7N"
   },
   "outputs": [],
   "source": [
    "# training hyper_parameters\n",
    "lr = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "### TODO: initialize the model and the optimizer\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, optimizer, train_loader)\n",
    "    train_losses.extend(train_loss)\n",
    "    train_accuracies.extend(train_accuracy)\n",
    "    test_loss, test_accuracy = test(model, test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4CQLCYm5g6_"
   },
   "outputs": [],
   "source": [
    "### TODO: Visualize training curves "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3_Modified.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
