{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdpJ06_akp1I"
   },
   "source": [
    "# Lab 4: Introduction to Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqRO7oE1wzIe"
   },
   "source": [
    "## 1 Numpy Edge Detection\n",
    "To build intuition about convolutions we begin by implementing an image edge detection filter in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZ3-DTgKygLA"
   },
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and visualize a sample image\n",
    "camera = data.camera()\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(camera, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emIzd-Brwww7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the edge detection kernel\n",
    "# kernel(numpy.array): kernel for edge detection with size of (3*3)\n",
    "kernel = np.array([[-1, -1, -1],\n",
    "                   [-1, 8, -1],\n",
    "                   [-1, -1, -1]])\n",
    "kernel = kernel / 8.0\n",
    "print(f'Edge detection kernel:\\n\\n {kernel}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NLABDwqy9wa"
   },
   "source": [
    "### 1.1 Numpy 2D Convolution\n",
    "Write a double for loop to convolve the edge detection kernel with the image. Apply the filter with `stride=2`. Plot the absolute value of the edge detection output using matplotlib's `imshow`.\n",
    "Your final output should look like the image below.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?id=11fFJ3QrbF87w8ChZF45p-5uXtmxZU8Qv\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGMEste7y9WJ"
   },
   "outputs": [],
   "source": [
    "# height and width of the image\n",
    "H, W = camera.shape\n",
    "\n",
    "# stride of the convolution\n",
    "stride = 2\n",
    "\n",
    "# edge detection kernel size\n",
    "kernel_size = kernel.shape[0]\n",
    "\n",
    "\n",
    "### TODO: convolve edge detection kernel with the camera image\n",
    "\n",
    "\n",
    "### TODO: plot absolute value of the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKcNworvn3rF"
   },
   "source": [
    "## 2 PyTorch Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nftg-rvkz7Z"
   },
   "source": [
    "Now let's take a look at `torch.nn.conv2d`. Run the cell below to convolve 5 random kernels on the camera man image and see the shapes of the parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CexK0N_zoKyH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# initialize convolutional kernel\n",
    "conv_nn = torch.nn.Conv2d(1, 5, kernel_size=3, stride=2)\n",
    "\n",
    "# set the kernel bias to zero\n",
    "conv_nn.bias.data.zero_()\n",
    "\n",
    "# convert camera image to a torch.tensor of shape (1, 1, H, W)\n",
    "img_in = torch.tensor(camera, dtype=torch.float32)[None, None, :, :]\n",
    "\n",
    "# forward pass\n",
    "filtered_camera = conv_nn(img_in)\n",
    "\n",
    "print(f'output shape (batch_size, in_channels, H, W): {filtered_camera.shape}')\n",
    "print(f'kernel shape (out_channels, in_channels, kernel_size[0], kernel_size[1]): {conv_nn.weight.data.shape}\\n')\n",
    "\n",
    "# to compute the output keep in mind these variables and the formula for H,W output in torch.nn.Conv2d\n",
    "print('Convolution layer parameters:')\n",
    "print(f'Dilation: {conv_nn.dilation}')\n",
    "print(f'Stride: {conv_nn.stride}')\n",
    "print(f'Padding: {conv_nn.padding}')\n",
    "print(f'Kernel size: {conv_nn.kernel_size}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAgAIcSUoN8u"
   },
   "source": [
    "### 2.1 Functional 2D Convolution\n",
    "\n",
    "Consider a minibatch of a randomly generated images (`toy_train_images`). Pass these images through the randomly initialized convolutional layer above.\n",
    "\n",
    "Take the weights from the convolution layer above and implement the convolution using a double for loop.\n",
    "\n",
    "**Note**: By default, PyTorch uses channels first representation of images $(N, C, H, W)$ as opposed to $(N, H, W, C)$, where $N=$ number of samples, $H=$ image height, $W=$ image width, and $C=$ number of image channels, e.g. 3 for rgb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2-z1IlcoNc7"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "# toy minibatch hyperparameters\n",
    "mini_batch = 10\n",
    "height, width = (12, 12)\n",
    "in_channels = 1\n",
    "out_channels = 5\n",
    "\n",
    "# generating minibatch from uniform distribution\n",
    "toy_train_images = torch.rand(mini_batch, in_channels, height, width) \n",
    "\n",
    "\n",
    "### TODO: Copy the weights from previous cell's convolution layer\n",
    "### Hint: You can use copy.deepcopy\n",
    "my_weights = None\n",
    "\n",
    "def my_conv_nn(X, kernel_weights):\n",
    "    \"\"\"Uses a double for loop to convolve the input image `x` with `my_weights`\n",
    "    with a fixed stride of 2.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): a minibatch of images of shape (batch_size, in_channels, H, W)\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Convolution result\n",
    "\n",
    "    Shape:\n",
    "        - X: Of shape (N, C_in, H_in, W_in)\n",
    "        - kernel_weights: Of shape (C_out, C_in, 3, 3)\n",
    "        - output: (N, C_out, H_out, W_out)\n",
    "    \"\"\"\n",
    "    # convolution hyperparameters\n",
    "    H, W = (height, width)\n",
    "    stride = 2\n",
    "\n",
    "    ### TODO: Use for loop to implement a convolution\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2YINZXPLlD7"
   },
   "source": [
    "Confirm your custom function has the same behavior as `torch.nn.Conv2d` on the camera image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ec9ZgEiCL01D"
   },
   "outputs": [],
   "source": [
    "my_out = my_conv_nn(toy_train_images, my_weights)\n",
    "torch_out = conv_nn(toy_train_images)\n",
    "assert my_weights.shape == (5, 1, 3, 3), f\"Incorrect shape for 'my_weights' ({my_weights.shape}).\"\n",
    "assert torch.is_tensor(my_out), \"Your function output is not a torch.Tensor\"\n",
    "assert my_out.shape == torch_out.shape, f\"Incorrect output shape ({my_out.shape}).\"\n",
    "assert torch.norm(my_out - torch_out) < 1e-3, \"Incorrect function output values compared to torch module\"\n",
    "print('Well done! Your function has the same behaviour as torch.nn.Conv2d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfguPYBp2Ufx"
   },
   "source": [
    "### 2.2 Modular 2D Convolution\n",
    "\n",
    "Build a small convnet using `torch.nn.Module` with two layers and forward pass the astronaut image through it.<br> You do not need to train the model for this excercise. You should use the `torch.nn.Conv2d` for this part.\n",
    "\n",
    "The convnet should have the following specifications:<br>\n",
    "\n",
    "* Activation Function: `ReLU` <br>\n",
    "* Layer1: filter size `(5,5)`, out_channels `16`, Stride `2` convolution layer <br>\n",
    "* Layer2: `(2,2)` pooling layer <br>\n",
    "* Layer3: filter size `(3,3)`, out_channels `32` convolution layer <br>\n",
    "* Layer4: Linear layer with output of `5`\n",
    "\n",
    "\n",
    "In your forward function add print statements to show the size of the image at each layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8Br0woUaq4O"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from skimage import data\n",
    "\n",
    "# load and the astronaut image\n",
    "astronaut_np = data.astronaut()\n",
    "print(f\"astronaut.shape: {astronaut_np.shape}\")\n",
    "\n",
    "# visualize the original and preprocessed astronaut image\n",
    "# fig, ax = plt.subplots(1, 1)\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"Astronaut\")\n",
    "plt.imshow(astronaut_np)\n",
    "plt.axis('off')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiFhWp3dbx7H"
   },
   "source": [
    "Run below cell to convert the astronaut image into a tensor and reshape it into the shape that PyTorch expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9X4epQqa7da"
   },
   "outputs": [],
   "source": [
    "# convert the astronaut image to torch.tensor\n",
    "astronaut = torch.tensor(astronaut_np, dtype=torch.float32)\n",
    "\n",
    "# torch convolutions expect channels first representation\n",
    "# of shape (N, C, H, W)\n",
    "astronaut = astronaut.permute(2, 0, 1).unsqueeze(0)\n",
    "print(f'astronaut.shape: {astronaut.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9TFJEKd2TrQ"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functionalize as F\n",
    "from skimage import data\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        ### TODO: Define layers based on description above\n",
    "        self.conv1 = None\n",
    "        self.pool = None\n",
    "        self.conv2 = None\n",
    "        self.linear = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### TODO: Compelete forward pass, print image size after each layer\n",
    "        pass\n",
    "\n",
    "astronaut = data.astronaut()\n",
    "print(f\"astronaut.shape: {astronaut.shape}\")\n",
    "\n",
    "model = MyModel()\n",
    "model(astronaut_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcWmzjPSST7e"
   },
   "source": [
    "## 3 Pretrained AlexNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM6u1MrCqj_9"
   },
   "source": [
    "\n",
    "In this section, we will visualize a subset of the first layer filters of AlexNet and the result of applying these filters to the astronaut image.\n",
    "\n",
    "Run the below cell to download the trained [AlexNet](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) model using [PyTorch Hub](https://pytorch.org/docs/stable/hub.html)'s [`torch.hub.load()`](https://pytorch.org/docs/stable/hub.html#torch.hub.load) method. The model is switched to `eval()` mode since we will not be doing any training in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DHYhLB4-hCp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# load the alexnet model using pytorch hub from:\n",
    "# https://github.com/pytorch/vision/blob/winbuild/v0.6.0/torchvision/models/alexnet.py\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "\n",
    "# switch the model to \"eval\" mode since we are not doing any further training\n",
    "model.eval()\n",
    "\n",
    "# print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWYkBYSJSb3a"
   },
   "source": [
    "Since we are using a pretrained model, we need to make sure that our data has a similar distribution to the training data that the model was trained on. For our case here, this means that we need to preprocess the data in a similar manner to how it was done in the [original training pipeline](https://github.com/pytorch/examples/blob/97304e232807082c2e7b54c597615dc0ad8f6173/imagenet/main.py#L197-L198).\n",
    "\n",
    "Run the below cell to preprocess and visualize the astronaut image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSllLIyRSgRm"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from skimage import data\n",
    "\n",
    "def image_normalizer(image):\n",
    "    r\"\"\"Normalizes the input to scale [0 1].\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray or torch.Tensor): image to be rescaled\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray or torch.Tensor): rescaled image\n",
    "\n",
    "    Shape:\n",
    "        - image: (*) Any shape\n",
    "        - output: Same shape as input\n",
    "    \"\"\"\n",
    "    return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "# the mean and standard deviations of ImageNet dataset \n",
    "# that were used for preprocessing AlexNet training data\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# preprocess the astronaut image from the part 2\n",
    "astronaut_processed = astronaut / 255.0\n",
    "astronaut_processed = (astronaut_processed - mean[None, :, None, None]) / std[None, :, None, None]\n",
    "\n",
    "# visualize the original and preprocessed astronaut image\n",
    "astro_processed_np = astronaut_processed.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[0].imshow(astronaut_np)\n",
    "ax[0].axis('off')\n",
    "ax[1].set_title(\"Preprocessed Image\")\n",
    "ax[1].imshow(image_normalizer(astro_processed_np))\n",
    "ax[1].axis('off')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igZoc0opSju9"
   },
   "source": [
    "### 3.1 Visualizing AlexNet Kernels\n",
    "\n",
    "The kernels (filters) in the first layer of AlexNet are of size 11. Visualize a randomly selected subset of 20 of these first layer filters as well as the respective output of convolving each kernel with the astronaut image. Your answer will look something like this\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?id=1azdohWuo3EEO9KC0szZmJOywX54jXlPz\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STIP9HcIq8jY"
   },
   "outputs": [],
   "source": [
    "# random seed\n",
    "np.random.seed(691)\n",
    "\n",
    "# get the weights of the first layer's kernels of the model\n",
    "# https://github.com/pytorch/vision/blob/9dff1b40ee9741216686556cc59fbf16964c8156/torchvision/models/alexnet.py#L18\n",
    "conv_sequential = model.features\n",
    "conv0 = conv_sequential[0]\n",
    "conv0_weights = conv0.weight\n",
    "\n",
    "# indices of kernels to show\n",
    "random_inds = np.random.permutation(64)[0:20]\n",
    "\n",
    "### TODO: convolve the astronaut image with the kernel weights and obtain the outputs\n",
    "\n",
    "\n",
    "### TODO: plot 10 kernels corresponding to the 10 indices in `random_inds` and \n",
    "### their convolution outputs. You may use the provided `image_normalizer()`\n",
    "### function in above cell for scaling the kernel weights and outputs\n",
    "### for visualization.\n",
    "for i, ind in enumerate(random_inds):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_4_modified.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
