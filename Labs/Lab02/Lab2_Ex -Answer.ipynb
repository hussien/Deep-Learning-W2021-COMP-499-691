{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S02d9PimAZKJ"
   },
   "source": [
    "In this lab we will go over over basic stochastic optimization and uses in pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIDPxv5l2qpI"
   },
   "source": [
    "(1) Setup the MNIST dataloaders for both the training (and  now as well test) set as in Lab 1 part 2. You do not need to iterate through the dataloaders, these will be used in the rest of the lab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HIv6h0xwUWyx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b37eea3a7cf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x= torch.randn(5 , requires_grad=True)\n",
    "y=x**2 \n",
    "y.backward()\n",
    "y=x**3 \n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 256\n",
    "batch_size_test = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14MBuiusdZmA"
   },
   "source": [
    "(2) Consider your work from [Lab 1 part 2 (2)]. Modify the neural network as follows. The hidden outputs should be size 100 in both hidden layers and the activations should be relu. Modify the final layer to output 10 values instead of 1. \n",
    "\n",
    "Your code should implement $f(x) = W_2\\rho(W_1\\rho(W_0x+b_0)+b_1)+b_2$ with $f: R^{786}-> R^{10}$ and $\\rho = relu$\n",
    "\n",
    "Initialize the weights  using a variant of xavier intialization $w_{ij} \\sim N(0,\\frac{1}{\\sqrt{n_i}})$ where $n_i$ is the size of the layer input. Initialize the biases as 0. Write a helper function to perform this initialization for subsequent parts of this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aejr5YkMlJZN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14483055,  0.20985911,  0.15247459, ..., -0.0756262 ,\n",
       "         0.11198171,  0.02941895],\n",
       "       [-0.12859168,  0.2728823 ,  0.04559954, ..., -0.02268817,\n",
       "         0.0892945 ,  0.04191412],\n",
       "       [ 0.19581739, -0.05347473, -0.11650499, ...,  0.13047089,\n",
       "        -0.04795322,  0.16375934],\n",
       "       ...,\n",
       "       [ 0.06831523,  0.06099567, -0.05386164, ...,  0.11614697,\n",
       "         0.06132986,  0.06845243],\n",
       "       [ 0.12989187,  0.06402156,  0.10899956, ...,  0.18078908,\n",
       "        -0.12812119,  0.20618821],\n",
       "       [ 0.0550139 , -0.02729148, -0.20291188, ...,  0.22803449,\n",
       "         0.08416595, -0.03106789]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xavier(m,h):\n",
    "    return np.random.randn(m,h)*np.math.sqrt(1/m)\n",
    "xavier(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict={\n",
    "    # print(W1)\n",
    "    \"W1\": torch.randn((20, 10), requires_grad=True)\n",
    "    \"W2\": torch.randn((20, 1), requires_grad=True)\n",
    "    # define the bias terms\n",
    "    \"B1\":torch.randn((20), requires_grad=True)\n",
    "    \"B2\":torch.randn((20), requires_grad=True)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bhZ0I_q7dnb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF 256 images Time= : 0.7837040959999939\n"
     ]
    }
   ],
   "source": [
    "## Define the network\n",
    "def my_nn(input,param_dict): \n",
    "    W1=param_dict[\"W1\"]\n",
    "    W2=param_dict[\"W2\"]\n",
    "    B1=param_dict[\"B1\"]\n",
    "    features = torch.flatten(input).reshape(-1)\n",
    "        # calculate hidden and output layers\n",
    "    h1 = torch.tanh_((features @ W0) + B0)\n",
    "    h2 = torch.relu_((h1 @ W1) + B1)\n",
    "    output = h2 @ W2 + B2\n",
    "#     output=(output - 2)/0.5\n",
    "#     print(output)\n",
    "#     print(output.reshape(-1,1))\n",
    "    return output\n",
    "#      return F.log_softmax(output.reshape(-1,1), dim=1)\n",
    "\n",
    "start = timeit.default_timer()   \n",
    "for idx in range(len(example_data)):\n",
    "     my_nn(example_data[idx][0],param_dict)\n",
    "stop = timeit.default_timer()\n",
    "print('FF 256 images Time= :',stop - start) \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTDfIZSOk1mF"
   },
   "source": [
    "We will evaluate the cross entropy loss and average accuracy on this randomly initialized dataset. Use the torch.nn.functional.cross_entropy() function to compute the loss  $\\frac{1}{N}\\sum_i^N CrossEntropy(f(x_i),y_i)$ and accuracy. Your accuracy should be close to $10\\%$ as the network has random weights. Note the pytorch cross_entropy function already applies the softmax operator so you do not need to apply this just feed the model output directly  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "def cross_entropy(p, q):\n",
    "    return -sum([p[i]*log2(q[i]) for i in range(len(p))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "vq3Zh5A_lBOV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  0  loss= tensor(2.3805)  acc= 0.07421875\n",
      "batch:  1  loss= tensor(2.3521)  acc= 0.06640625\n",
      "batch:  2  loss= tensor(2.3536)  acc= 0.09375\n",
      "batch:  3  loss= tensor(2.3348)  acc= 0.09375\n",
      "batch:  4  loss= tensor(2.3574)  acc= 0.046875\n",
      "batch:  5  loss= tensor(2.3695)  acc= 0.0546875\n",
      "batch:  6  loss= tensor(2.3816)  acc= 0.0703125\n",
      "batch:  7  loss= tensor(2.3238)  acc= 0.078125\n",
      "batch:  8  loss= tensor(2.3502)  acc= 0.05078125\n",
      "batch:  9  loss= tensor(2.3966)  acc= 0.07421875\n",
      "batch:  10  loss= tensor(2.3653)  acc= 0.08203125\n",
      "batch:  11  loss= tensor(2.3879)  acc= 0.07421875\n",
      "batch:  12  loss= tensor(2.3749)  acc= 0.0859375\n",
      "batch:  13  loss= tensor(2.3650)  acc= 0.0703125\n",
      "batch:  14  loss= tensor(2.3397)  acc= 0.07421875\n",
      "batch:  15  loss= tensor(2.4153)  acc= 0.06640625\n",
      "batch:  16  loss= tensor(2.3088)  acc= 0.1015625\n",
      "batch:  17  loss= tensor(2.3553)  acc= 0.08203125\n",
      "batch:  18  loss= tensor(2.3379)  acc= 0.078125\n",
      "batch:  19  loss= tensor(2.3313)  acc= 0.08203125\n",
      "batch:  20  loss= tensor(2.3762)  acc= 0.078125\n",
      "batch:  21  loss= tensor(2.3561)  acc= 0.07421875\n",
      "batch:  22  loss= tensor(2.3854)  acc= 0.0625\n",
      "batch:  23  loss= tensor(2.3090)  acc= 0.0859375\n",
      "batch:  24  loss= tensor(2.3969)  acc= 0.08203125\n",
      "batch:  25  loss= tensor(2.3838)  acc= 0.07421875\n",
      "batch:  26  loss= tensor(2.3404)  acc= 0.0859375\n",
      "batch:  27  loss= tensor(2.3645)  acc= 0.0859375\n",
      "batch:  28  loss= tensor(2.3830)  acc= 0.07421875\n",
      "batch:  29  loss= tensor(2.3501)  acc= 0.0703125\n",
      "batch:  30  loss= tensor(2.3562)  acc= 0.06640625\n",
      "batch:  31  loss= tensor(2.3401)  acc= 0.11328125\n",
      "batch:  32  loss= tensor(2.3551)  acc= 0.0859375\n",
      "batch:  33  loss= tensor(2.3483)  acc= 0.08203125\n",
      "batch:  34  loss= tensor(2.3716)  acc= 0.06640625\n",
      "batch:  35  loss= tensor(2.3247)  acc= 0.0546875\n",
      "batch:  36  loss= tensor(2.3241)  acc= 0.08984375\n",
      "batch:  37  loss= tensor(2.3871)  acc= 0.0546875\n",
      "batch:  38  loss= tensor(2.3914)  acc= 0.07421875\n",
      "batch:  39  loss= tensor(2.4287)  acc= 0.0\n",
      "avg acc = 0.0740234375\n",
      "avg loss = tensor(2.3613)\n"
     ]
    }
   ],
   "source": [
    "from itertools import repeat \n",
    "cuda = torch.cuda.is_available()\n",
    "acc=[]\n",
    "loss=[]\n",
    "batch_idx=0\n",
    "for data, targets in test_loader:\n",
    "  #move to GPU if available\n",
    "#   if cuda:\n",
    "#     data, target = data.to('cuda'), target.to('cuda')\n",
    "  #compute model output\n",
    "#     print(len(data))\n",
    "#     for i in range(len(data)):\n",
    "    Q=torch.zeros(len(data), 10)\n",
    "    for i in range(len(data)):\n",
    "        out=my_nn(data[i],param_dict)\n",
    "#         print(out.numpy())\n",
    "        Q[i,:]=out\n",
    "#         print(len(targets))\n",
    "  #comptue accuracy for minibatch\n",
    "    pred = torch.nn.functional.softmax(Q, dim=1)\n",
    "    total=0\n",
    "    for i, p in enumerate(pred):\n",
    "        if targets[i] == torch.max(p.data, 0)[1]:\n",
    "            total = total + 1\n",
    "    acc.append(total/len(data))\n",
    "  #compute loss for minibatch\n",
    "    loss.append(torch.nn.functional.cross_entropy(Q,targets))\n",
    "    print('batch: ',batch_idx,' loss=',loss[batch_idx],' acc=',acc[batch_idx])\n",
    "    batch_idx=batch_idx+1\n",
    "#aggregate loss and accuracy for all test data\n",
    "print('avg acc =',sum(acc)/len(acc))\n",
    "print('avg loss =',sum(loss)/len(loss))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqKxv88wTYFo"
   },
   "source": [
    "(3) (a) Without any use of the torch.optim package implement from scratch mini-batch Stochastic Gradient Descent training to minimize the loss  $\\frac{1}{N}\\sum_i^N CrossEntropy(f(x_i),y_i)$ over the MNIST dataset. Use a minibatch size of 128 and a learning rate of 0.01 and run training for 20 epochs.  \n",
    "\n",
    "You will use torch autograd features (e.g. .backward()) to obtain the gradients given each mini-batch at each parameter and then modify the existing parameters based on this gradient. \n",
    "\n",
    "Store the losses and training of each minibatch and plot each of these (with iterations (not epochs) as the x-axis). You can optionally smooth out these plots over 20-100 iteration window of your choosing to make them cleaner to read. Compute and report the final test accuracy as well at the end of the 20 epochs. \n",
    "\n",
    "You should end up with 2 plots and a final test accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4Lw5vyAegCC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "train_losses = [] # use to append the avg loss for each minibatch \n",
    "train_accs = [] # use to append the avg acc of minibatch\n",
    "\n",
    "alpha=0.01\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "  for ... #Iterate over dataset\n",
    "    #Compute the gradient for the minibatch\n",
    "\n",
    "    #Update the model parameters by w_t-alpha*\\grad_{w_t}(f(w_t))\n",
    "\n",
    "    #The following code will clear the gradient buffers for the next iteration\n",
    "    for (_,param) in param_dict.items():\n",
    "      if param.grad is not None: \n",
    "        param.grad.detach_() \n",
    "        param.grad.zero_()\n",
    "    \n",
    "    #Update loss and acc tracking \n",
    "\n",
    "#Plot the train loss and acc\n",
    "\n",
    "#Evaluate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYqVuiN0dK6z"
   },
   "source": [
    "(b) Modify the above code to perform minibatch SGD with momentum. Use a momentum of $\\mu=0.9$ and learning rate $\\alpha=0.01$. Use the following formulation of momentum: \n",
    "\n",
    "$g = \\nabla_wCE(f(w_t,X),Y)$  gradient estimate with mini-batch\n",
    "\n",
    "$v_{t+1} = \\mu * v_t + g$ \n",
    "\n",
    "$w_{t+1} = w - \\alpha*v_{t+1}$ \n",
    "\n",
    "Obtain the same plots as before and a final test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAI69yxnrA-S"
   },
   "outputs": [],
   "source": [
    "#Reinitialize your network to random!\n",
    "initialize_nn(param_dict)\n",
    "\n",
    "losses = [] # use to append the avg loss for each minibatch \n",
    "train_acc = [] # use to append the avg acc of minibatch\n",
    "\n",
    "alpha=0.01\n",
    "mu=0.9\n",
    "\n",
    "for epoch in range(20):\n",
    "  for ... #Iterate over dataset\n",
    "    #Compute the gradient for the minibatch\n",
    "\n",
    "    #Update the model parameters as described above\n",
    "\n",
    "    #The following code will clear the gradient buffers for the next iteration\n",
    "    for (_,param) in param_dict.items():\n",
    "      if param.grad is not None: \n",
    "        param.grad.detach_() \n",
    "        param.grad.zero_()\n",
    "\n",
    "    #Update loss and acc tracking \n",
    "\n",
    "#Plot the train loss and acc\n",
    "\n",
    "#Evaluate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mo_i-btpZw-m"
   },
   "source": [
    "(4) Repeat (3) but now using the package torch.optim.SGD to perform SGD (a) and SGD with momentum (b). Use the same learning rate in the case of (a) and the same learning rate and momentum in the case of (b)\n",
    "\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html?highlight=torch%20optim%20sgd#torch.optim.SGD\n",
    "\n",
    "\n",
    "Plot the same training curves and accuracy curves. Overlay this with their respective implementation from (3). Report the final accuracy.\n",
    "\n",
    "You should end up with 4 plots (train acc,train loss)x(sgd, sgd+momentum) each containing two curves (your implementation from (3) and the torch.optim result). Due to random initialization and random training order you will not get identical results to (3) but the curves and final accuracies should end up relatively close. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCnQoLCXrpBt"
   },
   "outputs": [],
   "source": [
    "#Answer for SGD\n",
    "import torch\n",
    "#Make sure to reinitialize your network to random before starting training\n",
    "initialize_nn(param_dict)\n",
    "\n",
    "#optim.SGD takes a list of parameters which you can get from your dictionary as follows\n",
    "parameter_list = param_dict.values()\n",
    "\n",
    "optimizer = torch.optim.SGD(parameter_list, lr=alpha)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fy8p8DUIkT8m"
   },
   "outputs": [],
   "source": [
    "#Answer for SGD+momentum"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab2_Ex.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
