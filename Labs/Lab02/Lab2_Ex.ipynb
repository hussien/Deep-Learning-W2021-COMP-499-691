{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S02d9PimAZKJ"
   },
   "source": [
    "In this lab we will go over over basic stochastic optimization and uses in pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIDPxv5l2qpI"
   },
   "source": [
    "(1) Setup the MNIST dataloaders for both the training (and  now as well test) set as in Lab 1 part 2. You do not need to iterate through the dataloaders, these will be used in the rest of the lab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIv6h0xwUWyx"
   },
   "outputs": [],
   "source": [
    "#This cell does not require outputing anything but is setup for subsequent cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14MBuiusdZmA"
   },
   "source": [
    "(2) Consider your work from [Lab 1 part 2 (2)]. Modify the neural network as follows. The hidden outputs should be size 100 in both hidden layers and the activations should be relu. Modify the final layer to output 10 values instead of 1. \n",
    "\n",
    "Your code should implement $f(x) = W_2\\rho(W_1\\rho(W_0x+b_0)+b_1)+b_2$ with $f: R^{786}-> R^{10}$ and $\\rho = relu$\n",
    "\n",
    "Initialize the weights  using a variant of xavier intialization $w_{ij} \\sim N(0,\\frac{1}{\\sqrt{n_i}})$ where $n_i$ is the size of the layer input. Initialize the biases as 0. Write a helper function to perform this initialization for subsequent parts of this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aejr5YkMlJZN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhZ0I_q7dnb9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## Initialize and track the parameters using a list or dictionary\n",
    "param_dict = {\n",
    "    \"W0\": None, \n",
    "    \"b0\": None,\n",
    "    \"W1\": None,\n",
    "    \"b1\": None,\n",
    "    \"W2\": None,\n",
    "    \"b2\": None,\n",
    "    }\n",
    "\n",
    "## Define the network\n",
    "def my_nn(input, param_dict):\n",
    "    r\"\"\"Performs a single forward pass of a Neural Network with the given \n",
    "    parameters in param_dict.\n",
    "\n",
    "    Args:\n",
    "        input (torch.tensor): Batch of images of shape (B, H, W), where B is \n",
    "            the number of input samples,and H and W are the image height and \n",
    "            width respectively.\n",
    "        param_dict (dict of torch.tensor): Dictionary containing the parameters\n",
    "            of the neural network. Expects dictionary keys to be of format \n",
    "            \"W#\" and \"b#\" where # is the layer number.\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: Neural network output of shape (B, )\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def initialize_nn(param_dict)\n",
    "  \"\"\"Takes a dictionary with existing torch tensors \n",
    "      and re-initializes them using xavier initialization\n",
    "  \"\"\"\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTDfIZSOk1mF"
   },
   "source": [
    "We will evaluate the cross entropy loss and average accuracy on this randomly initialized dataset. Use the torch.nn.functional.cross_entropy() function to compute the loss  $\\frac{1}{N}\\sum_i^N CrossEntropy(f(x_i),y_i)$ and accuracy. Your accuracy should be close to $10\\%$ as the network has random weights. Note the pytorch cross_entropy function already applies the softmax operator so you do not need to apply this just feed the model output directly  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vq3Zh5A_lBOV"
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "for data, targets in test_dataloader:\n",
    "  #move to GPU if available\n",
    "  if cuda:\n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "\n",
    "  #compute model output\n",
    "\n",
    "  #comptue accuracy for minibatch\n",
    "\n",
    "  #compute loss for minibatch\n",
    "\n",
    "#aggregate loss and accuracy for all test data\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqKxv88wTYFo"
   },
   "source": [
    "(3) (a) Without any use of the torch.optim package implement from scratch mini-batch Stochastic Gradient Descent training to minimize the loss  $\\frac{1}{N}\\sum_i^N CrossEntropy(f(x_i),y_i)$ over the MNIST dataset. Use a minibatch size of 128 and a learning rate of 0.01 and run training for 20 epochs.  \n",
    "\n",
    "You will use torch autograd features (e.g. .backward()) to obtain the gradients given each mini-batch at each parameter and then modify the existing parameters based on this gradient. \n",
    "\n",
    "Store the losses and training of each minibatch and plot each of these (with iterations (not epochs) as the x-axis). You can optionally smooth out these plots over 20-100 iteration window of your choosing to make them cleaner to read. Compute and report the final test accuracy as well at the end of the 20 epochs. \n",
    "\n",
    "You should end up with 2 plots and a final test accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4Lw5vyAegCC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "train_losses = [] # use to append the avg loss for each minibatch \n",
    "train_accs = [] # use to append the avg acc of minibatch\n",
    "\n",
    "alpha=0.01\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "  for ... #Iterate over dataset\n",
    "    #Compute the gradient for the minibatch\n",
    "\n",
    "    #Update the model parameters by w_t-alpha*\\grad_{w_t}(f(w_t))\n",
    "\n",
    "    #The following code will clear the gradient buffers for the next iteration\n",
    "    for (_,param) in param_dict.items():\n",
    "      if param.grad is not None: \n",
    "        param.grad.detach_() \n",
    "        param.grad.zero_()\n",
    "    \n",
    "    #Update loss and acc tracking \n",
    "\n",
    "#Plot the train loss and acc\n",
    "\n",
    "#Evaluate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYqVuiN0dK6z"
   },
   "source": [
    "(b) Modify the above code to perform minibatch SGD with momentum. Use a momentum of $\\mu=0.9$ and learning rate $\\alpha=0.01$. Use the following formulation of momentum: \n",
    "\n",
    "$g = \\nabla_wCE(f(w_t,X),Y)$  gradient estimate with mini-batch\n",
    "\n",
    "$v_{t+1} = \\mu * v_t + g$ \n",
    "\n",
    "$w_{t+1} = w - \\alpha*v_{t+1}$ \n",
    "\n",
    "Obtain the same plots as before and a final test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAI69yxnrA-S"
   },
   "outputs": [],
   "source": [
    "#Reinitialize your network to random!\n",
    "initialize_nn(param_dict)\n",
    "\n",
    "losses = [] # use to append the avg loss for each minibatch \n",
    "train_acc = [] # use to append the avg acc of minibatch\n",
    "\n",
    "alpha=0.01\n",
    "mu=0.9\n",
    "\n",
    "for epoch in range(20):\n",
    "  for ... #Iterate over dataset\n",
    "    #Compute the gradient for the minibatch\n",
    "\n",
    "    #Update the model parameters as described above\n",
    "\n",
    "    #The following code will clear the gradient buffers for the next iteration\n",
    "    for (_,param) in param_dict.items():\n",
    "      if param.grad is not None: \n",
    "        param.grad.detach_() \n",
    "        param.grad.zero_()\n",
    "\n",
    "    #Update loss and acc tracking \n",
    "\n",
    "#Plot the train loss and acc\n",
    "\n",
    "#Evaluate on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mo_i-btpZw-m"
   },
   "source": [
    "(4) Repeat (3) but now using the package torch.optim.SGD to perform SGD (a) and SGD with momentum (b). Use the same learning rate in the case of (a) and the same learning rate and momentum in the case of (b)\n",
    "\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html?highlight=torch%20optim%20sgd#torch.optim.SGD\n",
    "\n",
    "\n",
    "Plot the same training curves and accuracy curves. Overlay this with their respective implementation from (3). Report the final accuracy.\n",
    "\n",
    "You should end up with 4 plots (train acc,train loss)x(sgd, sgd+momentum) each containing two curves (your implementation from (3) and the torch.optim result). Due to random initialization and random training order you will not get identical results to (3) but the curves and final accuracies should end up relatively close. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCnQoLCXrpBt"
   },
   "outputs": [],
   "source": [
    "#Answer for SGD\n",
    "import torch\n",
    "#Make sure to reinitialize your network to random before starting training\n",
    "initialize_nn(param_dict)\n",
    "\n",
    "#optim.SGD takes a list of parameters which you can get from your dictionary as follows\n",
    "parameter_list = param_dict.values()\n",
    "\n",
    "optimizer = torch.optim.SGD(parameter_list, lr=alpha)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fy8p8DUIkT8m"
   },
   "outputs": [],
   "source": [
    "#Answer for SGD+momentum"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab2_Ex.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
