{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1_part2_Ex.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S02d9PimAZKJ"
      },
      "source": [
        "In this lab we will review some basics of pytorch. Save your answers for this lab as they will be used for part of Lab 2. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIDPxv5l2qpI"
      },
      "source": [
        "(1) Create a dataloader for the MNIST training data using torchvision package. Have your dataloader iterate over the training set outputing mini-batches of size 256 image samples. Note you do not need to use the image labels in this lab. You may follow the example in the official pytorch examples: \n",
        "\n",
        "https://github.com/pytorch/examples/blob/master/mnist/main.py#L112-L120\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14MBuiusdZmA"
      },
      "source": [
        "(2) Using only torch primitives (e.g. torch.matmul, torch._relu, etc) implement a simple feedforward neural network with 2 hidden layers that takes as input MNIST digits and outputs a single scalar value. You may select the hidden layer width (greater than 20) and activations (tanh, relu, sigmoid, others) as desired.  Initialize the weights and biases with uniform random values in the range -1 to 1. Avoid using any functions from torch.nn class. Using the loop from (1) Forward pass through the dataset in mini-batches of 256 and record the time this takes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhZ0I_q7dnb9"
      },
      "source": [
        "import torch\n",
        "\n",
        "## Initialize and track the parameters using a list or dictionary\n",
        "param_dict = {\n",
        "    \"W0\": None,\n",
        "    \"b0\": None,\n",
        "    \"W1\": None,\n",
        "    \"b1\": None,\n",
        "    \"W2\": None,\n",
        "    \"b2\": None,\n",
        "    }\n",
        "\n",
        "## Define the network\n",
        "def my_nn(input, param_dict):\n",
        "    r\"\"\"Performs a single forward pass of a Neural Network with the given \n",
        "    parameters in param_dict.\n",
        "\n",
        "    Args:\n",
        "        input (torch.tensor): Batch of images of shape (B, H, W), where B is \n",
        "            the number of input samples,and H and W are the image height and \n",
        "            width respectively.\n",
        "        param_dict (dict of torch.tensor): Dictionary containing the parameters\n",
        "            of the neural network. Expects dictionary keys to be of format \n",
        "            \"W#\" and \"b#\" where # is the layer number.\n",
        "\n",
        "    Returns:\n",
        "        torch.tensor: Neural network output of shape (B, )\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "## Perform forward pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndXpk3C5bwQ8"
      },
      "source": [
        "(3) Implement a new torch.nn.module that performs the equivalent of the network in (2). Initialize it with the same weights and validate the outputs of this network is the same as the one in (2) on MNIST training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbgzjuqtlhvx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4WyZ8PFFuMT"
      },
      "source": [
        "(4) For a batch of 256 random samples, compute the gradient of the average of the neural network outputs (over the batch) w.r.t to the weights using torch autograd. Compute the gradients for the torch.nn based model in (3) and validate the gradients match those from those computed with (2). \n",
        "\n",
        "**Note**: The network here is $f: \\mathcal{R}^{HW}\\rightarrow\\mathcal{R}$, with $256$ samples you should obtain $o=\\frac{1}{256}\\sum_{i=0}^{255}f(x_i)$. You are asked to find $\\nabla_w o$ for all the parameters $w$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WoNwefYGPWL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YHbWOIQHKow"
      },
      "source": [
        "(5) Perform  the forward and backward passes from (3), 10 times on cpu and 10 times on gpu, report the average time for both. Repeat this for just the forward pass. In the end you should obtain 4 average run times (forward and backward, forward only) x (cpu, gpu) "
      ]
    }
  ]
}