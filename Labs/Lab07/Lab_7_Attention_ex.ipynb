{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMvnHJy2VAh7"
   },
   "source": [
    "# Self-Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnc8ygodCf88"
   },
   "source": [
    "In this lab, we will try to gain insight into the self-attention operation using the sequential MNIST example from before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcGa4Zk7CSO6"
   },
   "source": [
    "## 0 Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nafW18o5aPa9"
   },
   "source": [
    "Run the code cell below to download the MNIST digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hotfix for very recent MNIST download issue https://github.com/pytorch/vision/issues/1938 \n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "###\n",
    "\n",
    "dataset = torchvision.datasets.MNIST('./', download=True, transform=transforms.Compose([transforms.ToTensor()]), train=True)\n",
    "train_indices = torch.arange(0, 10000)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "\n",
    "dataset=torchvision.datasets.MNIST('./', download=True, transform=transforms.Compose([transforms.ToTensor()]), train=False)\n",
    "test_indices = torch.arange(0, 10000)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ-OsDod2fvG"
   },
   "source": [
    "## 1 Self-Attention without Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w7IP5RKSqOz"
   },
   "source": [
    "In this section, will implement a very simple model based on self-attention without positional encoding. The model you will implement will consider the input image as a sequence of 28 rows. You may use PyTorch's [`nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) for this part. Implement a model with the following architecture:\n",
    "\n",
    "* **Input**: Input image of shape `(batch_size, sequence_length, input_size)`, where $\\text{sequence_length} = \\text{image_height}$ and $\\text{input_size} = \\text{image_width}$.\n",
    "\n",
    "* **Linear 1**: Linear layer which converts input of shape `(sequence_length*batch_size, input_size)` to input of shape `(sequence_length*batch_size, embed_dim)`, where `embed_dim` is the embedding dimension.\n",
    "\n",
    "* **Attention 1**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`.\n",
    "\n",
    "* **ReLU**: ReLU activation layer.\n",
    "\n",
    "* **Linear 2**: Linear layer which converts input of shape `(sequence_length*batch_size, embed_dim)` to input of shape `(sequence_length*batch_size, embed_dim)`.\n",
    "\n",
    "* **ReLU**: ReLU activation layer.\n",
    "\n",
    "* **Attention 2**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`.\n",
    "\n",
    "* **ReLU**: ReLU activation layer.\n",
    "\n",
    "* **AvgPool**: Average along the sequence dimension from `(batch_size, sequence_length, features_dim)` to `(batch_size, features_dim)`\n",
    "\n",
    "* **Linear 3**: Linear layer which takes an input of shape `(batch_size, sequence_length*embed_dim)` and outputs the class logits of shape `(batch_size, 10)`.\n",
    "\n",
    "\n",
    "**NOTE**: Be cautious of correctly permuting and reshaping the input between layers. E.g. if `x` is of shape `(batch_size, sequence_length, input_size)`, note that `x.reshape(batch_size*sequence_length, -1) != x.permute(1,0,2).reshape(batch_size*sequence_length, -1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead_Attn(nn.Module):\n",
    "    def __init__(self, embed_dim ,num_head ):        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim ##1024\n",
    "        self.num_head = num_head ##8\n",
    "        self.head_dim = self.embed_dim // self.num_head\n",
    "        \n",
    "        \n",
    "        assert self.embed_dim%self.num_head == 0\n",
    "        \n",
    "        self.q = nn.Linear(self.embed_dim , self.embed_dim )\n",
    "        self.k = nn.Linear(self.embed_dim , self.embed_dim )\n",
    "        self.v = nn.Linear(self.embed_dim , self.embed_dim)\n",
    "        \n",
    "        self.f_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(.35)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "    \n",
    "    def forward(self, x ):\n",
    "        batch_size = x.shape[0]\n",
    "        src_len    = x.shape[1]\n",
    "#         print('src_len=',src_len)\n",
    "        \n",
    "        w_k = self.k(x)\n",
    "        w_q = self.q(x)        \n",
    "        w_v = self.v(x)\n",
    "        \n",
    "        w_q = w_q.view(batch_size,-1,self.head_dim)\n",
    "        w_k = w_k.view(batch_size,-1,self.head_dim)\n",
    "        w_v = w_v.view(batch_size,-1,self.head_dim)\n",
    "         \n",
    "        energy = torch.matmul( w_k.permute(0,2,1) ,w_q )\n",
    "        energy = energy/self.scale\n",
    "        energy = torch.softmax(energy,-1)\n",
    "        \n",
    "        \n",
    "        f_energy = torch.matmul( self.dropout(energy) , w_v.permute(0,2,1))\n",
    "        f_energy = f_energy.permute(0, 2, 1)\n",
    "        f_energy = f_energy.reshape(batch_size,-1)\n",
    "        out = self.f_linear(f_energy)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "ppwhZ19Ff9FC"
   },
   "outputs": [],
   "source": [
    "# Self-attention without positional encoding\n",
    "torch.manual_seed(691)\n",
    "\n",
    "# Define your model here\n",
    "class myModel(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, seq_length,\n",
    "                 num_classes=10, num_heads=8):\n",
    "        super(myModel, self).__init__()\n",
    "        # TODO: Initialize myModel\n",
    "        self.input_size = input_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.num_classes = num_classes\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_size, embed_dim)\n",
    "        self.attention1=MultiHead_Attn(embed_dim, 8)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attention2=MultiHead_Attn(embed_dim, 8)\n",
    "        self.linear3 = nn.Linear(embed_dim*seq_length, 10)     \n",
    "        self.avgpool=nn.AvgPool2d((seq_length, 1), stride=(2, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        # TODO: Implement myModel forward pass\n",
    "        batch_size, sequence_length, input_size = x.shape\n",
    "        input=x.reshape(batch_size*sequence_length, -1)\n",
    "        l1_out=self.linear1(input)\n",
    "        a1_out=self.attention1(l1_out) \n",
    "        relu1_out=self.relu(a1_out)        \n",
    "#         print(type(relu1_out))\n",
    "        l2_out=self.linear2(relu1_out)\n",
    "#         print(l2_out.shape)\n",
    "        relu2_out=self.relu(l2_out)\n",
    "        a2_out=self.attention2(relu2_out)\n",
    "#         print(a2_out.shape)\n",
    "        relu3_out=self.relu(a2_out)\n",
    "#         print(relu3_out.shape)\n",
    "        relu3_out=relu3_out.reshape(batch_size,sequence_length, -1) \n",
    "#         print(relu3_out.shape)\n",
    "        avgpool_out=self.avgpool(relu3_out)\n",
    "        avgpool_out=avgpool_out.reshape(batch_size, -1) \n",
    "#         print(avgpool_out.shape)\n",
    "        l3_out=self.linear2(avgpool_out)\n",
    "        return l3_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zu1c88kgKDZ"
   },
   "source": [
    "Train and evaluate your model by running the cell below. Expect to see  `60-80%` test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "z3FlSD16S8Nh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [10/157], Loss: 2.5169\n",
      "Epoch [1/8], Step [20/157], Loss: 2.3786\n",
      "Epoch [1/8], Step [30/157], Loss: 2.3539\n",
      "Epoch [1/8], Step [40/157], Loss: 2.3043\n",
      "Epoch [1/8], Step [50/157], Loss: 2.3285\n",
      "Epoch [1/8], Step [60/157], Loss: 2.2978\n",
      "Epoch [1/8], Step [70/157], Loss: 2.0004\n",
      "Epoch [1/8], Step [80/157], Loss: 1.9677\n",
      "Epoch [1/8], Step [90/157], Loss: 2.1236\n",
      "Epoch [1/8], Step [100/157], Loss: 1.7003\n",
      "Epoch [1/8], Step [110/157], Loss: 1.9901\n",
      "Epoch [1/8], Step [120/157], Loss: 1.6480\n",
      "Epoch [1/8], Step [130/157], Loss: 1.8560\n",
      "Epoch [1/8], Step [140/157], Loss: 1.8463\n",
      "Epoch [1/8], Step [150/157], Loss: 1.6737\n",
      "Epoch [2/8], Step [10/157], Loss: 1.3322\n",
      "Epoch [2/8], Step [20/157], Loss: 1.6528\n",
      "Epoch [2/8], Step [30/157], Loss: 1.6002\n",
      "Epoch [2/8], Step [40/157], Loss: 1.7638\n",
      "Epoch [2/8], Step [50/157], Loss: 1.5760\n",
      "Epoch [2/8], Step [60/157], Loss: 1.5664\n",
      "Epoch [2/8], Step [70/157], Loss: 1.4938\n",
      "Epoch [2/8], Step [80/157], Loss: 1.5565\n",
      "Epoch [2/8], Step [90/157], Loss: 1.5405\n",
      "Epoch [2/8], Step [100/157], Loss: 1.5736\n",
      "Epoch [2/8], Step [110/157], Loss: 1.5054\n",
      "Epoch [2/8], Step [120/157], Loss: 1.2671\n",
      "Epoch [2/8], Step [130/157], Loss: 1.4646\n",
      "Epoch [2/8], Step [140/157], Loss: 1.2533\n",
      "Epoch [2/8], Step [150/157], Loss: 1.5371\n",
      "Epoch [3/8], Step [10/157], Loss: 1.4050\n",
      "Epoch [3/8], Step [20/157], Loss: 1.0369\n",
      "Epoch [3/8], Step [30/157], Loss: 1.2893\n",
      "Epoch [3/8], Step [40/157], Loss: 1.4952\n",
      "Epoch [3/8], Step [50/157], Loss: 1.5757\n",
      "Epoch [3/8], Step [60/157], Loss: 1.5483\n",
      "Epoch [3/8], Step [70/157], Loss: 1.2638\n",
      "Epoch [3/8], Step [80/157], Loss: 1.1697\n",
      "Epoch [3/8], Step [90/157], Loss: 1.2990\n",
      "Epoch [3/8], Step [100/157], Loss: 0.8806\n",
      "Epoch [3/8], Step [110/157], Loss: 0.9845\n",
      "Epoch [3/8], Step [120/157], Loss: 1.0830\n",
      "Epoch [3/8], Step [130/157], Loss: 1.1220\n",
      "Epoch [3/8], Step [140/157], Loss: 1.1152\n",
      "Epoch [3/8], Step [150/157], Loss: 1.1880\n",
      "Epoch [4/8], Step [10/157], Loss: 1.3257\n",
      "Epoch [4/8], Step [20/157], Loss: 1.0064\n",
      "Epoch [4/8], Step [30/157], Loss: 1.1558\n",
      "Epoch [4/8], Step [40/157], Loss: 1.3290\n",
      "Epoch [4/8], Step [50/157], Loss: 1.1791\n",
      "Epoch [4/8], Step [60/157], Loss: 1.0900\n",
      "Epoch [4/8], Step [70/157], Loss: 1.0315\n",
      "Epoch [4/8], Step [80/157], Loss: 1.1494\n",
      "Epoch [4/8], Step [90/157], Loss: 1.3993\n",
      "Epoch [4/8], Step [100/157], Loss: 1.3090\n",
      "Epoch [4/8], Step [110/157], Loss: 1.0155\n",
      "Epoch [4/8], Step [120/157], Loss: 1.0457\n",
      "Epoch [4/8], Step [130/157], Loss: 0.9468\n",
      "Epoch [4/8], Step [140/157], Loss: 1.3770\n",
      "Epoch [4/8], Step [150/157], Loss: 1.0364\n",
      "Epoch [5/8], Step [10/157], Loss: 1.3091\n",
      "Epoch [5/8], Step [20/157], Loss: 1.1745\n",
      "Epoch [5/8], Step [30/157], Loss: 1.3945\n",
      "Epoch [5/8], Step [40/157], Loss: 1.1823\n",
      "Epoch [5/8], Step [50/157], Loss: 1.2852\n",
      "Epoch [5/8], Step [60/157], Loss: 1.2442\n",
      "Epoch [5/8], Step [70/157], Loss: 0.9090\n",
      "Epoch [5/8], Step [80/157], Loss: 0.8563\n",
      "Epoch [5/8], Step [90/157], Loss: 0.9839\n",
      "Epoch [5/8], Step [100/157], Loss: 0.8785\n",
      "Epoch [5/8], Step [110/157], Loss: 0.8066\n",
      "Epoch [5/8], Step [120/157], Loss: 1.5008\n",
      "Epoch [5/8], Step [130/157], Loss: 0.9967\n",
      "Epoch [5/8], Step [140/157], Loss: 1.0386\n",
      "Epoch [5/8], Step [150/157], Loss: 1.0976\n",
      "Epoch [6/8], Step [10/157], Loss: 0.9835\n",
      "Epoch [6/8], Step [20/157], Loss: 1.0819\n",
      "Epoch [6/8], Step [30/157], Loss: 0.9429\n",
      "Epoch [6/8], Step [40/157], Loss: 1.1362\n",
      "Epoch [6/8], Step [50/157], Loss: 1.0435\n",
      "Epoch [6/8], Step [60/157], Loss: 1.0566\n",
      "Epoch [6/8], Step [70/157], Loss: 0.9276\n",
      "Epoch [6/8], Step [80/157], Loss: 0.8511\n",
      "Epoch [6/8], Step [90/157], Loss: 0.8713\n",
      "Epoch [6/8], Step [100/157], Loss: 0.9633\n",
      "Epoch [6/8], Step [110/157], Loss: 1.0507\n",
      "Epoch [6/8], Step [120/157], Loss: 0.9260\n",
      "Epoch [6/8], Step [130/157], Loss: 0.9487\n",
      "Epoch [6/8], Step [140/157], Loss: 1.0235\n",
      "Epoch [6/8], Step [150/157], Loss: 0.9021\n",
      "Epoch [7/8], Step [10/157], Loss: 0.9614\n",
      "Epoch [7/8], Step [20/157], Loss: 0.9864\n",
      "Epoch [7/8], Step [30/157], Loss: 0.8781\n",
      "Epoch [7/8], Step [40/157], Loss: 1.0833\n",
      "Epoch [7/8], Step [50/157], Loss: 0.9997\n",
      "Epoch [7/8], Step [60/157], Loss: 0.8864\n",
      "Epoch [7/8], Step [70/157], Loss: 0.9337\n",
      "Epoch [7/8], Step [80/157], Loss: 0.9632\n",
      "Epoch [7/8], Step [90/157], Loss: 0.9593\n",
      "Epoch [7/8], Step [100/157], Loss: 0.7020\n",
      "Epoch [7/8], Step [110/157], Loss: 1.0222\n",
      "Epoch [7/8], Step [120/157], Loss: 1.0243\n",
      "Epoch [7/8], Step [130/157], Loss: 1.0990\n",
      "Epoch [7/8], Step [140/157], Loss: 0.6862\n",
      "Epoch [7/8], Step [150/157], Loss: 0.9897\n",
      "Epoch [8/8], Step [10/157], Loss: 0.9542\n",
      "Epoch [8/8], Step [20/157], Loss: 0.9306\n",
      "Epoch [8/8], Step [30/157], Loss: 0.8751\n",
      "Epoch [8/8], Step [40/157], Loss: 0.9715\n",
      "Epoch [8/8], Step [50/157], Loss: 0.7755\n",
      "Epoch [8/8], Step [60/157], Loss: 0.8226\n",
      "Epoch [8/8], Step [70/157], Loss: 0.8874\n",
      "Epoch [8/8], Step [80/157], Loss: 0.8327\n",
      "Epoch [8/8], Step [90/157], Loss: 0.7186\n",
      "Epoch [8/8], Step [100/157], Loss: 0.8437\n",
      "Epoch [8/8], Step [110/157], Loss: 1.1434\n",
      "Epoch [8/8], Step [120/157], Loss: 0.9664\n",
      "Epoch [8/8], Step [130/157], Loss: 0.7147\n",
      "Epoch [8/8], Step [140/157], Loss: 0.9429\n",
      "Epoch [8/8], Step [150/157], Loss: 0.7495\n",
      "Test Accuracy of the model on the 10000 test images: 71.25 %\n"
     ]
    }
   ],
   "source": [
    "# Same training code \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 8\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Initialize model\n",
    "model = myModel(input_size=input_size, embed_dim=hidden_size, seq_length=sequence_length)\n",
    "model = model.to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "#         print(labels.shape)\n",
    "#         print(outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYeCqWt_BBRz"
   },
   "source": [
    "## 2 Self-Attention with Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l71kP10-45YF"
   },
   "source": [
    "Implement a similar model to part (1), except this time your embedded input should be concatenated with the positional encoding. For the purpose of this lab, we will use a learned positional encoding, which will be a trainable embedding. Your positional encodings will be added to the initial transformation of the input.\n",
    "\n",
    "* **Input**: Input image of shape `(batch_size, sequence_length, input_size)`, where $\\text{sequence_length} = \\text{image_height}$ and $\\text{input_size} = \\text{image_width}$.\n",
    "\n",
    "* **Linear 1**: Linear layer which converts input of shape `(batch_size*sequence_length, input_size)` to input of shape `(batch_size*sequence_length, embed_dim)`, where `embed_dim` is the embedding dimension.\n",
    "\n",
    "* **Add Positional Encoding**: Add a learnable positional enconding of shape `(sequence_length, batch_size, embed_dim)` to input of shape `(sequence_length, batch_size, embed_dim)`, where `pos_embed` is the positional embedding size. The output will be of shape `(sequence_length, batch_size, embed_dim)`.\n",
    "\n",
    "* **Attention 1**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`.\n",
    "\n",
    "* **ReLU**: ReLU activation layer.\n",
    "\n",
    "* **Linear 2**: Linear layer which converts input of shape `(sequence_length*batch_size, embed_dim)` to input of shape `(sequence_length*batch_size, embed_dim)`.\n",
    "\n",
    "\n",
    "* **Attention 2**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`.\n",
    "\n",
    "* **ReLU**: ReLU activation layer.\n",
    "\n",
    "* **AvgPool**: Average along the sequence dimension from `(batch_size, sequence_length, embed_dim)` to `(batch_size, embed_dim)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "-xAfP5H2_p6o"
   },
   "outputs": [],
   "source": [
    "# Self-attention without positional encoding\n",
    "torch.manual_seed(691)\n",
    "\n",
    "# Define your model here\n",
    "class myModel_pos(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, seq_length,\n",
    "                 num_classes=10, num_heads=8):\n",
    "        super(myModel_pos, self).__init__()\n",
    "        # TODO: Initialize myModel\n",
    "        self.input_size = input_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.num_classes = num_classes\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.positional_encoding = nn.Parameter(torch.rand(self.seq_length, self.input_size))\n",
    "        self.linear1 = nn.Linear(input_size, embed_dim)\n",
    "        self.attention1=MultiHead_Attn(embed_dim, 8)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attention2=MultiHead_Attn(embed_dim, 8)\n",
    "        self.linear3 = nn.Linear(embed_dim*seq_length, 10)     \n",
    "        self.avgpool=nn.AvgPool2d((seq_length, 1), stride=(2, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        # TODO: Implement myModel forward pass\n",
    "        batch_size, sequence_length, input_size = x.shape\n",
    "        for i in range(batch_size):\n",
    "            x[i]=x[i]+self.positional_encoding\n",
    "         \n",
    "        \n",
    "        input=x.reshape(batch_size*sequence_length, -1)        \n",
    "        l1_out=self.linear1(input)\n",
    "        a1_out=self.attention1(l1_out) \n",
    "        relu1_out=self.relu(a1_out)        \n",
    "#         print(type(relu1_out))\n",
    "        l2_out=self.linear2(relu1_out)\n",
    "#         print(l2_out.shape)\n",
    "        relu2_out=self.relu(l2_out)\n",
    "        a2_out=self.attention2(relu2_out)\n",
    "#         print(a2_out.shape)\n",
    "        relu3_out=self.relu(a2_out)\n",
    "#         print(relu3_out.shape)\n",
    "        relu3_out=relu3_out.reshape(batch_size,sequence_length, -1) \n",
    "#         print(relu3_out.shape)\n",
    "        avgpool_out=self.avgpool(relu3_out)\n",
    "        avgpool_out=avgpool_out.reshape(batch_size, -1) \n",
    "#         print(avgpool_out.shape)\n",
    "        l3_out=self.linear2(avgpool_out)\n",
    "        return l3_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 64])\n"
     ]
    }
   ],
   "source": [
    "p=nn.Parameter(torch.rand(28, 64))\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3IKf9WDBS4Y"
   },
   "source": [
    "Use the same training code as the one from part 1 to train your model. You may copy the training loop here. Expect to see close to `~90+%` test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "kxsHnOzXBk95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [10/157], Loss: 2.3809\n",
      "Epoch [1/15], Step [20/157], Loss: 2.3217\n",
      "Epoch [1/15], Step [30/157], Loss: 2.3399\n",
      "Epoch [1/15], Step [40/157], Loss: 2.3338\n",
      "Epoch [1/15], Step [50/157], Loss: 2.3359\n",
      "Epoch [1/15], Step [60/157], Loss: 2.3004\n",
      "Epoch [1/15], Step [70/157], Loss: 2.3477\n",
      "Epoch [1/15], Step [80/157], Loss: 2.3101\n",
      "Epoch [1/15], Step [90/157], Loss: 2.3444\n",
      "Epoch [1/15], Step [100/157], Loss: 2.3145\n",
      "Epoch [1/15], Step [110/157], Loss: 2.3092\n",
      "Epoch [1/15], Step [120/157], Loss: 2.3262\n",
      "Epoch [1/15], Step [130/157], Loss: 2.3195\n",
      "Epoch [1/15], Step [140/157], Loss: 2.2571\n",
      "Epoch [1/15], Step [150/157], Loss: 2.0634\n",
      "Epoch [2/15], Step [10/157], Loss: 2.1390\n",
      "Epoch [2/15], Step [20/157], Loss: 1.8546\n",
      "Epoch [2/15], Step [30/157], Loss: 1.9561\n",
      "Epoch [2/15], Step [40/157], Loss: 1.5295\n",
      "Epoch [2/15], Step [50/157], Loss: 1.6447\n",
      "Epoch [2/15], Step [60/157], Loss: 1.6268\n",
      "Epoch [2/15], Step [70/157], Loss: 1.4775\n",
      "Epoch [2/15], Step [80/157], Loss: 1.6287\n",
      "Epoch [2/15], Step [90/157], Loss: 1.5356\n",
      "Epoch [2/15], Step [100/157], Loss: 1.2293\n",
      "Epoch [2/15], Step [110/157], Loss: 1.3573\n",
      "Epoch [2/15], Step [120/157], Loss: 1.2911\n",
      "Epoch [2/15], Step [130/157], Loss: 1.2015\n",
      "Epoch [2/15], Step [140/157], Loss: 1.2681\n",
      "Epoch [2/15], Step [150/157], Loss: 1.2519\n",
      "Epoch [3/15], Step [10/157], Loss: 1.2392\n",
      "Epoch [3/15], Step [20/157], Loss: 1.1675\n",
      "Epoch [3/15], Step [30/157], Loss: 1.2775\n",
      "Epoch [3/15], Step [40/157], Loss: 1.1163\n",
      "Epoch [3/15], Step [50/157], Loss: 1.1770\n",
      "Epoch [3/15], Step [60/157], Loss: 1.2627\n",
      "Epoch [3/15], Step [70/157], Loss: 0.9154\n",
      "Epoch [3/15], Step [80/157], Loss: 0.9214\n",
      "Epoch [3/15], Step [90/157], Loss: 1.0988\n",
      "Epoch [3/15], Step [100/157], Loss: 0.7856\n",
      "Epoch [3/15], Step [110/157], Loss: 0.7391\n",
      "Epoch [3/15], Step [120/157], Loss: 1.0325\n",
      "Epoch [3/15], Step [130/157], Loss: 0.9927\n",
      "Epoch [3/15], Step [140/157], Loss: 0.8329\n",
      "Epoch [3/15], Step [150/157], Loss: 0.7294\n",
      "Epoch [4/15], Step [10/157], Loss: 1.0562\n",
      "Epoch [4/15], Step [20/157], Loss: 0.7436\n",
      "Epoch [4/15], Step [30/157], Loss: 0.6303\n",
      "Epoch [4/15], Step [40/157], Loss: 0.5647\n",
      "Epoch [4/15], Step [50/157], Loss: 0.7945\n",
      "Epoch [4/15], Step [60/157], Loss: 0.7355\n",
      "Epoch [4/15], Step [70/157], Loss: 0.6854\n",
      "Epoch [4/15], Step [80/157], Loss: 0.7833\n",
      "Epoch [4/15], Step [90/157], Loss: 0.6526\n",
      "Epoch [4/15], Step [100/157], Loss: 0.8945\n",
      "Epoch [4/15], Step [110/157], Loss: 0.5157\n",
      "Epoch [4/15], Step [120/157], Loss: 0.6335\n",
      "Epoch [4/15], Step [130/157], Loss: 0.6782\n",
      "Epoch [4/15], Step [140/157], Loss: 0.6885\n",
      "Epoch [4/15], Step [150/157], Loss: 0.7086\n",
      "Epoch [5/15], Step [10/157], Loss: 0.6421\n",
      "Epoch [5/15], Step [20/157], Loss: 0.6250\n",
      "Epoch [5/15], Step [30/157], Loss: 0.5396\n",
      "Epoch [5/15], Step [40/157], Loss: 0.7412\n",
      "Epoch [5/15], Step [50/157], Loss: 0.7954\n",
      "Epoch [5/15], Step [60/157], Loss: 0.8396\n",
      "Epoch [5/15], Step [70/157], Loss: 0.7070\n",
      "Epoch [5/15], Step [80/157], Loss: 0.6495\n",
      "Epoch [5/15], Step [90/157], Loss: 0.3598\n",
      "Epoch [5/15], Step [100/157], Loss: 0.5121\n",
      "Epoch [5/15], Step [110/157], Loss: 0.3943\n",
      "Epoch [5/15], Step [120/157], Loss: 0.3910\n",
      "Epoch [5/15], Step [130/157], Loss: 0.4647\n",
      "Epoch [5/15], Step [140/157], Loss: 0.3422\n",
      "Epoch [5/15], Step [150/157], Loss: 0.5517\n",
      "Epoch [6/15], Step [10/157], Loss: 0.4880\n",
      "Epoch [6/15], Step [20/157], Loss: 0.5752\n",
      "Epoch [6/15], Step [30/157], Loss: 0.5535\n",
      "Epoch [6/15], Step [40/157], Loss: 0.4481\n",
      "Epoch [6/15], Step [50/157], Loss: 0.7284\n",
      "Epoch [6/15], Step [60/157], Loss: 0.5501\n",
      "Epoch [6/15], Step [70/157], Loss: 0.4748\n",
      "Epoch [6/15], Step [80/157], Loss: 0.4822\n",
      "Epoch [6/15], Step [90/157], Loss: 0.4227\n",
      "Epoch [6/15], Step [100/157], Loss: 0.7819\n",
      "Epoch [6/15], Step [110/157], Loss: 0.5505\n",
      "Epoch [6/15], Step [120/157], Loss: 0.4995\n",
      "Epoch [6/15], Step [130/157], Loss: 0.5384\n",
      "Epoch [6/15], Step [140/157], Loss: 0.4378\n",
      "Epoch [6/15], Step [150/157], Loss: 0.5206\n",
      "Epoch [7/15], Step [10/157], Loss: 0.3289\n",
      "Epoch [7/15], Step [20/157], Loss: 0.6379\n",
      "Epoch [7/15], Step [30/157], Loss: 0.3283\n",
      "Epoch [7/15], Step [40/157], Loss: 0.4271\n",
      "Epoch [7/15], Step [50/157], Loss: 0.4683\n",
      "Epoch [7/15], Step [60/157], Loss: 0.5638\n",
      "Epoch [7/15], Step [70/157], Loss: 0.4205\n",
      "Epoch [7/15], Step [80/157], Loss: 0.6184\n",
      "Epoch [7/15], Step [90/157], Loss: 0.4659\n",
      "Epoch [7/15], Step [100/157], Loss: 0.6414\n",
      "Epoch [7/15], Step [110/157], Loss: 0.3406\n",
      "Epoch [7/15], Step [120/157], Loss: 0.4259\n",
      "Epoch [7/15], Step [130/157], Loss: 0.3278\n",
      "Epoch [7/15], Step [140/157], Loss: 0.4668\n",
      "Epoch [7/15], Step [150/157], Loss: 0.5011\n",
      "Epoch [8/15], Step [10/157], Loss: 0.5002\n",
      "Epoch [8/15], Step [20/157], Loss: 0.4879\n",
      "Epoch [8/15], Step [30/157], Loss: 0.6285\n",
      "Epoch [8/15], Step [40/157], Loss: 0.6124\n",
      "Epoch [8/15], Step [50/157], Loss: 0.4505\n",
      "Epoch [8/15], Step [60/157], Loss: 0.5732\n",
      "Epoch [8/15], Step [70/157], Loss: 0.3607\n",
      "Epoch [8/15], Step [80/157], Loss: 0.3387\n",
      "Epoch [8/15], Step [90/157], Loss: 0.6701\n",
      "Epoch [8/15], Step [100/157], Loss: 0.6387\n",
      "Epoch [8/15], Step [110/157], Loss: 0.5065\n",
      "Epoch [8/15], Step [120/157], Loss: 0.3656\n",
      "Epoch [8/15], Step [130/157], Loss: 0.8395\n",
      "Epoch [8/15], Step [140/157], Loss: 0.4246\n",
      "Epoch [8/15], Step [150/157], Loss: 0.5607\n",
      "Epoch [9/15], Step [10/157], Loss: 0.3730\n",
      "Epoch [9/15], Step [20/157], Loss: 0.5083\n",
      "Epoch [9/15], Step [30/157], Loss: 0.3999\n",
      "Epoch [9/15], Step [40/157], Loss: 0.4559\n",
      "Epoch [9/15], Step [50/157], Loss: 0.6846\n",
      "Epoch [9/15], Step [60/157], Loss: 0.6348\n",
      "Epoch [9/15], Step [70/157], Loss: 0.6705\n",
      "Epoch [9/15], Step [80/157], Loss: 0.3618\n",
      "Epoch [9/15], Step [90/157], Loss: 0.3790\n",
      "Epoch [9/15], Step [100/157], Loss: 0.3853\n",
      "Epoch [9/15], Step [110/157], Loss: 0.7393\n",
      "Epoch [9/15], Step [120/157], Loss: 0.3958\n",
      "Epoch [9/15], Step [130/157], Loss: 0.4558\n",
      "Epoch [9/15], Step [140/157], Loss: 0.3020\n",
      "Epoch [9/15], Step [150/157], Loss: 0.7041\n",
      "Epoch [10/15], Step [10/157], Loss: 0.3006\n",
      "Epoch [10/15], Step [20/157], Loss: 0.2955\n",
      "Epoch [10/15], Step [30/157], Loss: 0.5178\n",
      "Epoch [10/15], Step [40/157], Loss: 0.4833\n",
      "Epoch [10/15], Step [50/157], Loss: 1.2236\n",
      "Epoch [10/15], Step [60/157], Loss: 0.6087\n",
      "Epoch [10/15], Step [70/157], Loss: 0.7508\n",
      "Epoch [10/15], Step [80/157], Loss: 0.5057\n",
      "Epoch [10/15], Step [90/157], Loss: 0.5199\n",
      "Epoch [10/15], Step [100/157], Loss: 0.5405\n",
      "Epoch [10/15], Step [110/157], Loss: 0.6092\n",
      "Epoch [10/15], Step [120/157], Loss: 0.3723\n",
      "Epoch [10/15], Step [130/157], Loss: 0.4067\n",
      "Epoch [10/15], Step [140/157], Loss: 0.2451\n",
      "Epoch [10/15], Step [150/157], Loss: 0.5543\n",
      "Epoch [11/15], Step [10/157], Loss: 0.5108\n",
      "Epoch [11/15], Step [20/157], Loss: 0.2667\n",
      "Epoch [11/15], Step [30/157], Loss: 0.5179\n",
      "Epoch [11/15], Step [40/157], Loss: 0.3251\n",
      "Epoch [11/15], Step [50/157], Loss: 0.1705\n",
      "Epoch [11/15], Step [60/157], Loss: 0.5168\n",
      "Epoch [11/15], Step [70/157], Loss: 0.3248\n",
      "Epoch [11/15], Step [80/157], Loss: 0.2697\n",
      "Epoch [11/15], Step [90/157], Loss: 0.4176\n",
      "Epoch [11/15], Step [100/157], Loss: 0.4951\n",
      "Epoch [11/15], Step [110/157], Loss: 0.3623\n",
      "Epoch [11/15], Step [120/157], Loss: 0.3715\n",
      "Epoch [11/15], Step [130/157], Loss: 0.1256\n",
      "Epoch [11/15], Step [140/157], Loss: 0.3484\n",
      "Epoch [11/15], Step [150/157], Loss: 0.4650\n",
      "Epoch [12/15], Step [10/157], Loss: 0.3336\n",
      "Epoch [12/15], Step [20/157], Loss: 0.5885\n",
      "Epoch [12/15], Step [30/157], Loss: 0.3363\n",
      "Epoch [12/15], Step [40/157], Loss: 0.3494\n",
      "Epoch [12/15], Step [50/157], Loss: 0.7879\n",
      "Epoch [12/15], Step [60/157], Loss: 0.4695\n",
      "Epoch [12/15], Step [70/157], Loss: 0.4466\n",
      "Epoch [12/15], Step [80/157], Loss: 0.3855\n",
      "Epoch [12/15], Step [90/157], Loss: 0.2557\n",
      "Epoch [12/15], Step [100/157], Loss: 0.5303\n",
      "Epoch [12/15], Step [110/157], Loss: 0.6876\n",
      "Epoch [12/15], Step [120/157], Loss: 0.3864\n",
      "Epoch [12/15], Step [130/157], Loss: 0.3905\n",
      "Epoch [12/15], Step [140/157], Loss: 0.2623\n",
      "Epoch [12/15], Step [150/157], Loss: 0.2597\n",
      "Epoch [13/15], Step [10/157], Loss: 0.3164\n",
      "Epoch [13/15], Step [20/157], Loss: 0.3642\n",
      "Epoch [13/15], Step [30/157], Loss: 0.3067\n",
      "Epoch [13/15], Step [40/157], Loss: 0.4684\n",
      "Epoch [13/15], Step [50/157], Loss: 0.4583\n",
      "Epoch [13/15], Step [60/157], Loss: 0.3656\n",
      "Epoch [13/15], Step [70/157], Loss: 0.4527\n",
      "Epoch [13/15], Step [80/157], Loss: 0.4932\n",
      "Epoch [13/15], Step [90/157], Loss: 0.4289\n",
      "Epoch [13/15], Step [100/157], Loss: 0.4127\n",
      "Epoch [13/15], Step [110/157], Loss: 0.5347\n",
      "Epoch [13/15], Step [120/157], Loss: 0.5154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15], Step [130/157], Loss: 0.4715\n",
      "Epoch [13/15], Step [140/157], Loss: 0.5203\n",
      "Epoch [13/15], Step [150/157], Loss: 0.6086\n",
      "Epoch [14/15], Step [10/157], Loss: 0.4260\n",
      "Epoch [14/15], Step [20/157], Loss: 0.3986\n",
      "Epoch [14/15], Step [30/157], Loss: 0.3195\n",
      "Epoch [14/15], Step [40/157], Loss: 0.8566\n",
      "Epoch [14/15], Step [50/157], Loss: 0.3116\n",
      "Epoch [14/15], Step [60/157], Loss: 0.4197\n",
      "Epoch [14/15], Step [70/157], Loss: 0.8163\n",
      "Epoch [14/15], Step [80/157], Loss: 0.4062\n",
      "Epoch [14/15], Step [90/157], Loss: 0.6478\n",
      "Epoch [14/15], Step [100/157], Loss: 0.4344\n",
      "Epoch [14/15], Step [110/157], Loss: 0.4650\n",
      "Epoch [14/15], Step [120/157], Loss: 0.4384\n",
      "Epoch [14/15], Step [130/157], Loss: 0.5317\n",
      "Epoch [14/15], Step [140/157], Loss: 0.3790\n",
      "Epoch [14/15], Step [150/157], Loss: 0.4541\n",
      "Epoch [15/15], Step [10/157], Loss: 0.3259\n",
      "Epoch [15/15], Step [20/157], Loss: 0.6518\n",
      "Epoch [15/15], Step [30/157], Loss: 0.5384\n",
      "Epoch [15/15], Step [40/157], Loss: 0.3236\n",
      "Epoch [15/15], Step [50/157], Loss: 0.3991\n",
      "Epoch [15/15], Step [60/157], Loss: 0.4147\n",
      "Epoch [15/15], Step [70/157], Loss: 0.3977\n",
      "Epoch [15/15], Step [80/157], Loss: 0.4348\n",
      "Epoch [15/15], Step [90/157], Loss: 0.3023\n",
      "Epoch [15/15], Step [100/157], Loss: 0.7657\n",
      "Epoch [15/15], Step [110/157], Loss: 0.3700\n",
      "Epoch [15/15], Step [120/157], Loss: 0.2918\n",
      "Epoch [15/15], Step [130/157], Loss: 0.2377\n",
      "Epoch [15/15], Step [140/157], Loss: 0.3527\n",
      "Epoch [15/15], Step [150/157], Loss: 0.3052\n",
      "Test Accuracy of the model on the 10000 test images: 89.97 %\n"
     ]
    }
   ],
   "source": [
    "# Same training code \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 15\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Initialize model\n",
    "model = myModel_pos(input_size=input_size, embed_dim=hidden_size, seq_length=sequence_length)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_7 Attention_ex.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
